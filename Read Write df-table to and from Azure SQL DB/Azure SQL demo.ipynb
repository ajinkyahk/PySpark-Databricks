{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47fd8905-025b-44b0-b5df-7483da1a9284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EMP_ID</th><th>EMP_NAME</th><th>EMP_GENDER</th><th>EMP_CITY</th><th>EMP_DEPT_ID</th></tr></thead><tbody><tr><td>1</td><td>Nancy Davolio</td><td>F</td><td>Seattle</td><td>3</td></tr><tr><td>2</td><td>Andrew Fuller</td><td>M</td><td>Tacoma</td><td>2</td></tr><tr><td>3</td><td>Janet Leverling</td><td>F</td><td>Kirkland</td><td>1</td></tr><tr><td>4</td><td>Margaret Peacock</td><td>F</td><td>Redmond</td><td>3</td></tr><tr><td>5</td><td>Steven Buchanan</td><td>M</td><td>London</td><td>1</td></tr><tr><td>6</td><td>Robert King</td><td>M</td><td>London</td><td>2</td></tr><tr><td>7</td><td>Tony Stark</td><td>M</td><td>Mumbai</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Nancy Davolio",
         "F",
         "Seattle",
         3
        ],
        [
         2,
         "Andrew Fuller",
         "M",
         "Tacoma",
         2
        ],
        [
         3,
         "Janet Leverling",
         "F",
         "Kirkland",
         1
        ],
        [
         4,
         "Margaret Peacock",
         "F",
         "Redmond",
         3
        ],
        [
         5,
         "Steven Buchanan",
         "M",
         "London",
         1
        ],
        [
         6,
         "Robert King",
         "M",
         "London",
         2
        ],
        [
         7,
         "Tony Stark",
         "M",
         "Mumbai",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_GENDER",
         "type": "\"string\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_CITY",
         "type": "\"string\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_DEPT_ID",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS new_employees_table\n",
    "  USING JDBC\n",
    "OPTIONS (\n",
    "  url \"jdbc:sqlserver://server.database.windows.net:1403;database=db\",\n",
    "  dbtable \"tbl_employee\",\n",
    "  user 'user',\n",
    "  password 'password'\n",
    ");\n",
    "\n",
    "\n",
    "SELECT * FROM new_employees_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "302e7ee2-7599-479e-acc7-3508a51f8de1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>EMP_ID</td><td>int</td><td>null</td></tr><tr><td>EMP_NAME</td><td>string</td><td>null</td></tr><tr><td>EMP_GENDER</td><td>string</td><td>null</td></tr><tr><td>EMP_CITY</td><td>string</td><td>null</td></tr><tr><td>EMP_DEPT_ID</td><td>int</td><td>null</td></tr><tr><td></td><td></td><td></td></tr><tr><td># Detailed Table Information</td><td></td><td></td></tr><tr><td>Catalog</td><td>spark_catalog</td><td></td></tr><tr><td>Database</td><td>default</td><td></td></tr><tr><td>Table</td><td>new_employees_table</td><td></td></tr><tr><td>Owner</td><td>root</td><td></td></tr><tr><td>Created Time</td><td>Mon Oct 02 10:53:26 UTC 2023</td><td></td></tr><tr><td>Last Access</td><td>UNKNOWN</td><td></td></tr><tr><td>Created By</td><td>Spark 3.3.0</td><td></td></tr><tr><td>Type</td><td>EXTERNAL</td><td></td></tr><tr><td>Provider</td><td>JDBC</td><td></td></tr><tr><td>Location</td><td>jdbc:/JDBC</td><td></td></tr><tr><td>Serde Library</td><td>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</td><td></td></tr><tr><td>InputFormat</td><td>org.apache.hadoop.mapred.SequenceFileInputFormat</td><td></td></tr><tr><td>OutputFormat</td><td>org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</td><td></td></tr><tr><td>Storage Properties</td><td>[url=*********(redacted), dbtable=akdb.dbo.tbl_employee, user=*********(redacted), password=*********(redacted)]</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "EMP_ID",
         "int",
         null
        ],
        [
         "EMP_NAME",
         "string",
         null
        ],
        [
         "EMP_GENDER",
         "string",
         null
        ],
        [
         "EMP_CITY",
         "string",
         null
        ],
        [
         "EMP_DEPT_ID",
         "int",
         null
        ],
        [
         "",
         "",
         ""
        ],
        [
         "# Detailed Table Information",
         "",
         ""
        ],
        [
         "Catalog",
         "spark_catalog",
         ""
        ],
        [
         "Database",
         "default",
         ""
        ],
        [
         "Table",
         "new_employees_table",
         ""
        ],
        [
         "Owner",
         "root",
         ""
        ],
        [
         "Created Time",
         "Mon Oct 02 10:53:26 UTC 2023",
         ""
        ],
        [
         "Last Access",
         "UNKNOWN",
         ""
        ],
        [
         "Created By",
         "Spark 3.3.0",
         ""
        ],
        [
         "Type",
         "EXTERNAL",
         ""
        ],
        [
         "Provider",
         "JDBC",
         ""
        ],
        [
         "Location",
         "jdbc:/JDBC",
         ""
        ],
        [
         "Serde Library",
         "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
         ""
        ],
        [
         "InputFormat",
         "org.apache.hadoop.mapred.SequenceFileInputFormat",
         ""
        ],
        [
         "OutputFormat",
         "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
         ""
        ],
        [
         "Storage Properties",
         "[url=*********(redacted), dbtable=akdb.dbo.tbl_employee, user=*********(redacted), password=*********(redacted)]",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\":\"name of the column\"}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"data type of the column\"}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"comment of the column\"}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "describe extended new_employees_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8fc86ec-1c1b-46bb-b613-fa0148205e9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "insert into new_employees_table values (7, 'Ajinkya Kulkarni', 'M', 'Mumbai', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1067091c-0814-428c-b984-ca725c45249f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EMP_ID</th><th>EMP_NAME</th><th>EMP_GENDER</th><th>EMP_CITY</th><th>EMP_DEPT_ID</th></tr></thead><tbody><tr><td>1</td><td>Nancy Davolio</td><td>F</td><td>Seattle</td><td>3</td></tr><tr><td>2</td><td>Andrew Fuller</td><td>M</td><td>Tacoma</td><td>2</td></tr><tr><td>3</td><td>Janet Leverling</td><td>F</td><td>Kirkland</td><td>1</td></tr><tr><td>4</td><td>Margaret Peacock</td><td>F</td><td>Redmond</td><td>3</td></tr><tr><td>5</td><td>Steven Buchanan</td><td>M</td><td>London</td><td>1</td></tr><tr><td>6</td><td>Robert King</td><td>M</td><td>London</td><td>2</td></tr><tr><td>7</td><td>Ajinkya Kulkarni</td><td>M</td><td>Mumbai</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Nancy Davolio",
         "F",
         "Seattle",
         3
        ],
        [
         2,
         "Andrew Fuller",
         "M",
         "Tacoma",
         2
        ],
        [
         3,
         "Janet Leverling",
         "F",
         "Kirkland",
         1
        ],
        [
         4,
         "Margaret Peacock",
         "F",
         "Redmond",
         3
        ],
        [
         5,
         "Steven Buchanan",
         "M",
         "London",
         1
        ],
        [
         6,
         "Robert King",
         "M",
         "London",
         2
        ],
        [
         7,
         "Ajinkya Kulkarni",
         "M",
         "Mumbai",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_GENDER",
         "type": "\"string\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_CITY",
         "type": "\"string\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_DEPT_ID",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from new_employees_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d528d46d-838d-4ee0-9d9f-b3684ea1924f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EMP_ID</th><th>EMP_NAME</th><th>EMP_GENDER</th><th>EMP_CITY</th><th>EMP_DEPT_ID</th></tr></thead><tbody><tr><td>1</td><td>Nancy Davolio</td><td>F</td><td>Seattle</td><td>3</td></tr><tr><td>2</td><td>Andrew Fuller</td><td>M</td><td>Tacoma</td><td>2</td></tr><tr><td>3</td><td>Janet Leverling</td><td>F</td><td>Kirkland</td><td>1</td></tr><tr><td>4</td><td>Margaret Peacock</td><td>F</td><td>Redmond</td><td>3</td></tr><tr><td>5</td><td>Steven Buchanan</td><td>M</td><td>London</td><td>1</td></tr><tr><td>6</td><td>Robert King</td><td>M</td><td>London</td><td>2</td></tr><tr><td>7</td><td>Ajinkya Kulkarni</td><td>M</td><td>Mumbai</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Nancy Davolio",
         "F",
         "Seattle",
         3
        ],
        [
         2,
         "Andrew Fuller",
         "M",
         "Tacoma",
         2
        ],
        [
         3,
         "Janet Leverling",
         "F",
         "Kirkland",
         1
        ],
        [
         4,
         "Margaret Peacock",
         "F",
         "Redmond",
         3
        ],
        [
         5,
         "Steven Buchanan",
         "M",
         "London",
         1
        ],
        [
         6,
         "Robert King",
         "M",
         "London",
         2
        ],
        [
         7,
         "Ajinkya Kulkarni",
         "M",
         "Mumbai",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_GENDER",
         "type": "\"string\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_CITY",
         "type": "\"string\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "EMP_DEPT_ID",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "create or replace view empview as select * from new_employees_table;\n",
    "\n",
    "select * from empview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe47e7b-7e83-4afd-b21f-99f4a7b7c47a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: UPDATE destination only supports Delta sources.\n",
       "Some(Relation spark_catalog.default.new_employees_table[EMP_ID#50,EMP_NAME#51,EMP_GENDER#52,EMP_CITY#53,EMP_DEPT_ID#54] JDBCRelation(akdb.dbo.tbl_employee) [numPartitions=1]\n",
       ")\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.notADeltaSourceException(DeltaErrors.scala:527)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.notADeltaSourceException$(DeltaErrors.scala:525)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.notADeltaSourceException(DeltaErrors.scala:2509)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:320)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:354)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:31)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:87)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:81)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:331)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:154)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:304)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:303)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:147)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:337)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:763)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:337)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:334)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:141)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:141)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:133)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:106)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:104)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:820)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:815)\n",
       "\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:740)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:723)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:403)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:401)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:398)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:446)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:431)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:700)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:740)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:723)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:403)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:401)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:398)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:446)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:431)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:700)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: UPDATE destination only supports Delta sources.\nSome(Relation spark_catalog.default.new_employees_table[EMP_ID#50,EMP_NAME#51,EMP_GENDER#52,EMP_CITY#53,EMP_DEPT_ID#54] JDBCRelation(akdb.dbo.tbl_employee) [numPartitions=1]\n)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.notADeltaSourceException(DeltaErrors.scala:527)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.notADeltaSourceException$(DeltaErrors.scala:525)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.notADeltaSourceException(DeltaErrors.scala:2509)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:320)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:354)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:31)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:87)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:81)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:154)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:304)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:303)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:147)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:337)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:763)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:337)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:334)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:141)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:141)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:133)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:106)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:815)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:740)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:723)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:403)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:401)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:398)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:446)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:700)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:740)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:723)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:403)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:401)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:398)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:446)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:700)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "Error in SQL statement: DeltaAnalysisException: UPDATE destination only supports Delta sources.\nSome(Relation spark_catalog.default.new_employees_table[EMP_ID#50,EMP_NAME#51,EMP_GENDER#52,EMP_CITY#53,EMP_DEPT_ID#54] JDBCRelation(akdb.dbo.tbl_employee) [numPartitions=1]\n)",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "update empview set EMP_NAME='Tony Stark' where EMP_ID=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ed7f66-0e18-40e0-9f99-f1e37c612304",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "create table tbl_emp (emp_id int, emp_name string, emp_gender string, emp_city string, emp_dept_id int);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea82208e-b133-4752-869f-cffeedb8bc0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>7</td><td>7</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7,
         7
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "insert into tbl_emp select * from new_employees_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656941bd-d9fe-4744-8af2-99458eaf53c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>emp_name</th><th>emp_gender</th><th>emp_city</th><th>emp_dept_id</th></tr></thead><tbody><tr><td>1</td><td>Nancy Davolio</td><td>F</td><td>Seattle</td><td>3</td></tr><tr><td>2</td><td>Andrew Fuller</td><td>M</td><td>Tacoma</td><td>2</td></tr><tr><td>3</td><td>Janet Leverling</td><td>F</td><td>Kirkland</td><td>1</td></tr><tr><td>4</td><td>Margaret Peacock</td><td>F</td><td>Redmond</td><td>3</td></tr><tr><td>5</td><td>Steven Buchanan</td><td>M</td><td>London</td><td>1</td></tr><tr><td>6</td><td>Robert King</td><td>M</td><td>London</td><td>2</td></tr><tr><td>7</td><td>Daniel Craig</td><td>M</td><td>Mumbai</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Nancy Davolio",
         "F",
         "Seattle",
         3
        ],
        [
         2,
         "Andrew Fuller",
         "M",
         "Tacoma",
         2
        ],
        [
         3,
         "Janet Leverling",
         "F",
         "Kirkland",
         1
        ],
        [
         4,
         "Margaret Peacock",
         "F",
         "Redmond",
         3
        ],
        [
         5,
         "Steven Buchanan",
         "M",
         "London",
         1
        ],
        [
         6,
         "Robert King",
         "M",
         "London",
         2
        ],
        [
         7,
         "Daniel Craig",
         "M",
         "Mumbai",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "emp_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "emp_gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "emp_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "emp_dept_id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from tbl_emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef82e6a-e487-4091-8418-dc0a67a52ed1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th></tr></thead><tbody><tr><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "update tbl_emp set emp_name='Daniel Craig' where emp_id=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e5fd02-b08c-46b6-a0db-e8394df8e70b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "insert overwrite new_employees_table select * from tbl_emp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d75910d-781c-4014-bc9e-b59671fc5afd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_FEATURE.TABLE_OPERATION] The feature is not supported: Table `spark_catalog`.`default`.`new_employees_table` does not support DELETE. Please check the current catalog and namespace to make sure the qualified table name is expected, and also check the catalog implementation which is configured by \"spark.sql.catalog\".\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.operationOnlySupportedWithV2TableError(QueryCompilationErrors.scala:583)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:314)\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:69)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:69)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:100)\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:81)\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$4(QueryPlanner.scala:85)\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
       "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
       "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:82)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:100)\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:81)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:715)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:285)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:337)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:763)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:337)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:334)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:285)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:278)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:297)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:297)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:292)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:214)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:399)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:194)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:148)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:349)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:229)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:214)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:227)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:298)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:294)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:354)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:220)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:174)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:165)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:238)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:107)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:104)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:820)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:815)\n",
       "\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:740)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:723)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:403)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:401)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:398)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:446)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:431)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:700)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:740)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:723)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:403)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:401)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:398)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:446)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:431)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:700)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_FEATURE.TABLE_OPERATION] The feature is not supported: Table `spark_catalog`.`default`.`new_employees_table` does not support DELETE. Please check the current catalog and namespace to make sure the qualified table name is expected, and also check the catalog implementation which is configured by \"spark.sql.catalog\".\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.operationOnlySupportedWithV2TableError(QueryCompilationErrors.scala:583)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:314)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:69)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:69)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:100)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:81)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$4(QueryPlanner.scala:85)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:82)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:100)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:715)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:285)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:337)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:763)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:337)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:334)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:285)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:297)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:297)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:292)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:399)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:194)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:148)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:349)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:229)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:227)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:220)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:294)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:354)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:220)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:174)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:165)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:238)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:107)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:815)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:740)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:723)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:403)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:401)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:398)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:446)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:700)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:740)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:723)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:403)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:401)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:398)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:446)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:700)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "Error in SQL statement: AnalysisException: [UNSUPPORTED_FEATURE.TABLE_OPERATION] The feature is not supported: Table `spark_catalog`.`default`.`new_employees_table` does not support DELETE. Please check the current catalog and namespace to make sure the qualified table name is expected, and also check the catalog implementation which is configured by \"spark.sql.catalog\".",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "delete from new_employees_table where EMP_ID=7;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Azure SQL demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
