{"cells":[{"cell_type":"markdown","source":["#PySpark SQL Date and Timestamp Functions\n\n\n\n---\n\n\n**PySpark Date and Timestamp Functions are supported on DataFrame and SQL queries and they work similarly to traditional SQL, Date and Time are very important if you are using PySpark for ETL. Most of all these functions accept input as, Date type, Timestamp type, or String. If a String used, it should be in a default format that can be cast to date.**\n\n\n- DateType default format is yyyy-MM-dd \n\n- TimestampType default format is yyyy-MM-dd HH:mm:ss.SSSS\n\n- Returns null if the input is a string that can not be cast to Date or Timestamp.\n\n\n**PySpark SQL provides several Date & Timestamp functions hence keep an eye on and understand these. Always you should choose these functions instead of writing your own functions (UDF) as these functions are compile-time safe, handles null, and perform better when compared to PySpark UDF. If your PySpark application is critical on performance try to avoid using custom UDF at all costs as these are not guarantee performance.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dd8ebbb8-9ca9-4ed8-9e07-76c83d960653","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##PySpark SQL Date and Timestamp Functions Examples\n\n\n**Following are the most used PySpark SQL Date and Timestamp Functions with examples, you can use these on DataFrame and SQL expressions.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9be394b2-66ed-4994-8a34-af053001e40d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import *"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ac4a6d6-6f87-4bbc-aedf-6298a147118b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data = [\n    [\"1\",\"2020-02-01\"],[\"2\",\"2019-03-02\"],[\"3\",\"2021-03-01\"]\n]\n\ndf = spark.createDataFrame(data=data, schema=[\"id\", \"input\"])\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89cbf156-3859-43ed-ab2c-44bfad4869e7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- id: string (nullable = true)\n |-- input: string (nullable = true)\n\n+---+----------+\n|id |input     |\n+---+----------+\n|1  |2020-02-01|\n|2  |2019-03-02|\n|3  |2021-03-01|\n+---+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- id: string (nullable = true)\n |-- input: string (nullable = true)\n\n+---+----------+\n|id |input     |\n+---+----------+\n|1  |2020-02-01|\n|2  |2019-03-02|\n|3  |2021-03-01|\n+---+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##current_date()\n\n\n**Use current_date() to get the current system date. By default, the data will be returned in yyyy-dd-mm format.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c5542685-e5c9-4358-b0f2-d4afe3c1767a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(current_date().alias(\"current_date\")).show(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9ca7ed6-1475-4dcc-bc25-600e84a5798a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------+\n|current_date|\n+------------+\n|  2023-01-08|\n+------------+\nonly showing top 1 row\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------+\n|current_date|\n+------------+\n|  2023-01-08|\n+------------+\nonly showing top 1 row\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##date_format()\n\n\n**The below example uses date_format() to parses the date and converts from yyyy-mm-dd to MM-dd-yyyy format.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f7b68614-4a8e-4525-8ba3-1574cea961ad","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#date_format()\n\ndf.select( col(\"input\"), date_format( col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\") ).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25b36917-b428-4c62-910d-592b6b223ac0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+-----------+\n|     input|date_format|\n+----------+-----------+\n|2020-02-01| 02-01-2020|\n|2019-03-02| 03-02-2019|\n|2021-03-01| 03-01-2021|\n+----------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+-----------+\n|     input|date_format|\n+----------+-----------+\n|2020-02-01| 02-01-2020|\n|2019-03-02| 03-02-2019|\n|2021-03-01| 03-01-2021|\n+----------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##to_date()\n\n\n**Below example converts string in date format yyyy-MM-dd to a DateType yyyy-MM-dd using to_date(). You can also use this to convert into any specific format. PySpark supports all patterns supports on Java DateTimeFormatter.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40c14d44-6423-4bde-acbc-af9e4854ef01","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#to_date()\n\ndf2 =df.select(col(\"input\"), to_date(col(\"input\"), \"yyyy-MM-dd\").alias(\"to_date\"))\ndf.printSchema()\ndf2.printSchema()\ndf2.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"01cff772-f057-4dd4-bfce-01e2c1f0fbfe","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- id: string (nullable = true)\n |-- input: string (nullable = true)\n\nroot\n |-- input: string (nullable = true)\n |-- to_date: date (nullable = true)\n\n+----------+----------+\n|     input|   to_date|\n+----------+----------+\n|2020-02-01|2020-02-01|\n|2019-03-02|2019-03-02|\n|2021-03-01|2021-03-01|\n+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- id: string (nullable = true)\n |-- input: string (nullable = true)\n\nroot\n |-- input: string (nullable = true)\n |-- to_date: date (nullable = true)\n\n+----------+----------+\n|     input|   to_date|\n+----------+----------+\n|2020-02-01|2020-02-01|\n|2019-03-02|2019-03-02|\n|2021-03-01|2021-03-01|\n+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##datediff()\n\n\n**The below example returns the difference between two dates using datediff().**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5aa996e-3cb9-4f22-bde2-06ea615de79b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#datediff()\ndf.select(col(\"input\"), datediff(current_date(), col(\"input\")).alias(\"datediff\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eeeaf41c-49e6-4b87-ab4c-e88eb9b358eb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+--------+\n|     input|datediff|\n+----------+--------+\n|2020-02-01|    1072|\n|2019-03-02|    1408|\n|2021-03-01|     678|\n+----------+--------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+--------+\n|     input|datediff|\n+----------+--------+\n|2020-02-01|    1072|\n|2019-03-02|    1408|\n|2021-03-01|     678|\n+----------+--------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##months_between()\n\n\n**The below example returns the months between two dates using months_between().**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"476ec06d-6527-4f3d-b0d3-693509903933","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#months_between()\n\ndf.select(col(\"input\"), months_between(current_date(), col(\"input\")).alias(\"months_between\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b018828c-ed22-4801-8dff-6e78670c9c8e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+--------------+\n|     input|months_between|\n+----------+--------------+\n|2020-02-01|   35.22580645|\n|2019-03-02|   46.19354839|\n|2021-03-01|   22.22580645|\n+----------+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+--------------+\n|     input|months_between|\n+----------+--------------+\n|2020-02-01|   35.22580645|\n|2019-03-02|   46.19354839|\n|2021-03-01|   22.22580645|\n+----------+--------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##trunc()\n\n\n**The below example truncates the date at a specified unit using trunc().**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1b45850f-8723-423b-900b-bf1aa52b3a6d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#trunc()\n\ndf.select(col(\"input\"),\\\n         trunc(col(\"input\"), \"Month\").alias(\"month_trunc\"),\\\n         trunc(col(\"input\"), \"Year\").alias(\"year_trunc\")\\\n         ).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d33ee83-3b38-4d33-bdf1-82c01469b7e4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+-----------+----------+\n|     input|month_trunc|year_trunc|\n+----------+-----------+----------+\n|2020-02-01| 2020-02-01|2020-01-01|\n|2019-03-02| 2019-03-01|2019-01-01|\n|2021-03-01| 2021-03-01|2021-01-01|\n+----------+-----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+-----------+----------+\n|     input|month_trunc|year_trunc|\n+----------+-----------+----------+\n|2020-02-01| 2020-02-01|2020-01-01|\n|2019-03-02| 2019-03-01|2019-01-01|\n|2021-03-01| 2021-03-01|2021-01-01|\n+----------+-----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##add_months() , date_add(), date_sub()\n\n\n**Here we are adding and subtracting date and month from a given input.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c6221070-b2ef-4969-a7c6-dc0bfdab7d1e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#add_months(), date_add(), date_sub()\n\ndf.select(col(\"input\"),\\\n         add_months(col(\"input\"), 3).alias(\"add_months\"),\\\n         add_months(col(\"input\"), -3).alias(\"sub_months\"),\\\n         date_add(col(\"input\"), 5).alias(\"date_add\"),\\\n         date_sub(col(\"input\"), 5).alias(\"date_sub\")\\\n         ).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ac92de9f-a5fd-4b9d-9a80-21658b942657","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+----------+----------+----------+----------+\n|input     |add_months|sub_months|date_add  |date_sub  |\n+----------+----------+----------+----------+----------+\n|2020-02-01|2020-05-01|2019-11-01|2020-02-06|2020-01-27|\n|2019-03-02|2019-06-02|2018-12-02|2019-03-07|2019-02-25|\n|2021-03-01|2021-06-01|2020-12-01|2021-03-06|2021-02-24|\n+----------+----------+----------+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+----------+----------+----------+----------+\n|input     |add_months|sub_months|date_add  |date_sub  |\n+----------+----------+----------+----------+----------+\n|2020-02-01|2020-05-01|2019-11-01|2020-02-06|2020-01-27|\n|2019-03-02|2019-06-02|2018-12-02|2019-03-07|2019-02-25|\n|2021-03-01|2021-06-01|2020-12-01|2021-03-06|2021-02-24|\n+----------+----------+----------+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##year(), month(), month(),next_day(), weekofyear()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"323216ca-5630-419f-9586-3d575a6a78eb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(col(\"input\"),\\\n         year(col(\"input\")).alias(\"year\"),\\\n         month(col(\"input\")).alias(\"month\"),\\\n         next_day(col(\"input\"), \"Sunday\").alias(\"next_days\"),\\\n         weekofyear(col(\"input\")).alias(\"weekofyear\")\\\n         ).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ecd811f-33d3-4115-a8bc-b8ae21a4e8f6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+----+-----+----------+----------+\n|input     |year|month|next_days |weekofyear|\n+----------+----+-----+----------+----------+\n|2020-02-01|2020|2    |2020-02-02|5         |\n|2019-03-02|2019|3    |2019-03-03|9         |\n|2021-03-01|2021|3    |2021-03-07|9         |\n+----------+----+-----+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+----+-----+----------+----------+\n|input     |year|month|next_days |weekofyear|\n+----------+----+-----+----------+----------+\n|2020-02-01|2020|2    |2020-02-02|5         |\n|2019-03-02|2019|3    |2019-03-03|9         |\n|2021-03-01|2021|3    |2021-03-07|9         |\n+----------+----+-----+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##dayofweek(), dayofmonth(), dayofyear()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2dd617e5-39d7-4872-a26d-840d871b915d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(col(\"input\"),\\\n         dayofweek(col(\"input\")).alias(\"dayofweek\"),\\\n         dayofmonth(col(\"input\")).alias(\"dayofmonth\"),\\\n         dayofyear(col(\"input\")).alias(\"dayofyear\")\\\n         ).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12a48815-9c24-44c6-b5b8-6ea66e701e34","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+---------+----------+---------+\n|input     |dayofweek|dayofmonth|dayofyear|\n+----------+---------+----------+---------+\n|2020-02-01|7        |1         |32       |\n|2019-03-02|7        |2         |61       |\n|2021-03-01|2        |1         |60       |\n+----------+---------+----------+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+---------+----------+---------+\n|input     |dayofweek|dayofmonth|dayofyear|\n+----------+---------+----------+---------+\n|2020-02-01|7        |1         |32       |\n|2019-03-02|7        |2         |61       |\n|2021-03-01|2        |1         |60       |\n+----------+---------+----------+---------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#current_timestamp()\n\n**Following are the Timestamp Functions that you can use on SQL and on DataFrame. Let’s learn these with examples.**\n\n**Let’s create a test data.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f22dca2e-88d3-4660-99eb-f18c0c84090e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data = [[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n\ndf2 = spark.createDataFrame(data=data, schema=[\"id\", \"input\"])\ndf2.printSchema()\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e35c1f30-7307-4dfd-8d97-7bd16f7ac6fb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- id: string (nullable = true)\n |-- input: string (nullable = true)\n\n+---+-----------------------+\n|id |input                  |\n+---+-----------------------+\n|1  |02-01-2020 11 01 19 06 |\n|2  |03-01-2019 12 01 19 406|\n|3  |03-01-2021 12 01 19 406|\n+---+-----------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- id: string (nullable = true)\n |-- input: string (nullable = true)\n\n+---+-----------------------+\n|id |input                  |\n+---+-----------------------+\n|1  |02-01-2020 11 01 19 06 |\n|2  |03-01-2019 12 01 19 406|\n|3  |03-01-2021 12 01 19 406|\n+---+-----------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Below example returns the current timestamp in spark default format yyyy-MM-dd HH:mm:ss**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6761544e-25a8-4d78-904a-6b935fd5fdd3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#current_timestamp()\n\ndf2.select(current_timestamp().alias(\"current_timestamp\")).show(1,truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c22b75e3-7957-44aa-ad7a-4781fae92cee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------------+\n|current_timestamp      |\n+-----------------------+\n|2023-01-08 14:17:19.959|\n+-----------------------+\nonly showing top 1 row\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------------+\n|current_timestamp      |\n+-----------------------+\n|2023-01-08 14:17:19.959|\n+-----------------------+\nonly showing top 1 row\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##to_timestamp()\n\n\n**Converts string timestamp to Timestamp type format.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9bc11b10-0ee9-45b9-8d4b-538bb311392d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#to_timestamp()\n\ndf2.select(col(\"input\"),\\\n         to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\")\\\n         ).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2675055b-d372-4117-8ef2-8ae710dfc12f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------------+-----------------------+\n|input                  |to_timestamp           |\n+-----------------------+-----------------------+\n|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n+-----------------------+-----------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------------+-----------------------+\n|input                  |to_timestamp           |\n+-----------------------+-----------------------+\n|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n+-----------------------+-----------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##hour(), Minute() and second()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dbdf98b9-b17b-4dae-abd5-876d5c4be631","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#hour, minute, second\n\ndata = [[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n\ndf3 = spark.createDataFrame(data=data, schema=[\"id\", \"input\"])\n\ndf3.select(col(\"input\"),\\\n          hour(col(\"input\")).alias(\"hour\"),\\\n          minute(col(\"input\")).alias(\"minute\"),\\\n          second(col(\"input\")).alias(\"second\")\\\n          ).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f085631d-4fae-4e77-a9ec-32bdebe11299","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------------+----+------+------+\n|input                  |hour|minute|second|\n+-----------------------+----+------+------+\n|2020-02-01 11:01:19.06 |11  |1     |19    |\n|2019-03-01 12:01:19.406|12  |1     |19    |\n|2021-03-01 12:01:19.406|12  |1     |19    |\n+-----------------------+----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------------+----+------+------+\n|input                  |hour|minute|second|\n+-----------------------+----+------+------+\n|2020-02-01 11:01:19.06 |11  |1     |19    |\n|2019-03-01 12:01:19.406|12  |1     |19    |\n|2021-03-01 12:01:19.406|12  |1     |19    |\n+-----------------------+----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Conclusion:\n\n**In this post, I’ve consolidated the complete list of Date and Timestamp Functions with a description and example of some commonly used. You can find the complete list on the <a href=\"https://www.databricks.com/blog/2015/09/16/apache-spark-1-5-dataframe-api-highlights.html\">Blog</a>**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"77b9530f-f339-446c-bc38-bf49726acc2f","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark SQL Date and Timestamp Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2308953539647623}},"nbformat":4,"nbformat_minor":0}
