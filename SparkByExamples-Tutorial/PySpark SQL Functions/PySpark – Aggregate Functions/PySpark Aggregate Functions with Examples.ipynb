{"cells":[{"cell_type":"markdown","source":["#PySpark Aggregate Functions with Examples\n\n\n**PySpark provides built-in standard Aggregate functions defines in DataFrame API, these come in handy when we need to make aggregate operations on DataFrame columns. Aggregate functions operate on a group of rows and calculate a single return value for every group.**\n\n**All these aggregate functions accept input as, Column type or column name in a string and several other arguments based on the function and return Column type.**\n\n**When possible try to leverage standard library as they are little bit more compile-time safety, handles null and perform better when compared to UDF’s. If your application is critical on performance try to avoid using custom UDF at all costs as these are not guarantee on performance.**\n\n\n---\n\n\n##PySpark Aggregate Functions\n\n\n**PySpark SQL Aggregate functions are grouped as “agg_funcs” in Pyspark. Below is a list of functions defined under this group. Click on each link to learn with example.**\n\n\n- approx_count_distinct\n- avg\n- collect_list\n- collect_set\n- countDistinct\n- count\n- grouping\n- first\n- last\n- kurtosis\n- max\n- min\n- mean\n- skewness\n- stddev\n- stddev_samp\n- stddev_pop\n- sum\n- sumDistinct\n- variance, var_samp, var_pop\n\n\n---\n\n\n##PySpark Aggregate Functions Examples\n\n**First, let’s create a DataFrame to work with PySpark aggregate functions.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f17d51e6-f16d-4889-9340-8dd5c81c0ab9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["simpleData = [\n    (\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n]\n\nschema = [\"employee_name\", \"department\", \"Salary\"]\ndf = spark.createDataFrame(data=simpleData, schema=schema)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a063e82-9041-434d-93b9-24491f182296","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|Salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|Salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Now let’s see how to aggregate data in PySpark.**\n\n\n##approx_count_distinct Aggregate Function\n\n\n**In PySpark approx_count_distinct() function returns the count of distinct items in a group.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d27a5b1c-52b8-4add-9c66-47ce14bfd1c4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n#approx_count_distinct()\n\nprint(\"approx_count_distinct: \"+\\\n     str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6072360d-bba7-4189-88a3-dc25b33c9d67","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"approx_count_distinct: 6\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["approx_count_distinct: 6\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.select(approx_count_distinct(\"salary\")).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c6ef02fb-60cc-4c81-92c7-02de7d017bf8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[15]: [Row(approx_count_distinct(salary)=6)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[15]: [Row(approx_count_distinct(salary)=6)]"]}}],"execution_count":0},{"cell_type":"markdown","source":["##avg (average) Aggregate Function\n\n\n**avg() function returns the average of values in the input column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab81c8bb-5b59-468a-8827-a4f6e0e900f0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#avg\n\nprint(\"avg: \"+ str(df.select(avg(\"salary\")).collect()[0][0]))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"20bbbcd6-cf28-4864-9fb6-08fff0252991","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"avg: 3400.0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["avg: 3400.0\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##collect_list Aggregate Function\n\n**collect_list() function returns all values from an input column with duplicates.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"744f8011-a169-4b90-b903-33ce2af3369a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#collect_list\n\ndf.select(collect_list(\"salary\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c8907237-1093-42e7-b190-61a48a7d34d0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##collect_set Aggregate Function\n\n**collect_set() function returns all values from an input column with duplicate values eliminated.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49d94bf2-1939-418d-bfb2-7666f9298f71","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#collect_set\n\ndf.select(collect_set('salary')).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03c50260-adf1-44d3-a11b-8ae392b7fddb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##countDistinct Aggregate Function\n\n\n**countDistinct() function returns the number of distinct elements in a columns**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a6f650de-981a-4202-b915-ff6b90437652","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#countDistinct\n\ndf2 = df.select(count_distinct(\"department\", \"salary\"))\ndf2.show(truncate=False)\nprint(\"Distinct Count of Department & Salary: \"+ str(df2.collect()[0][0]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b48667a0-1741-4bc8-b05c-d67d57ff0145","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|8                                 |\n+----------------------------------+\n\nDistinct Count of Department & Salary: 8\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|8                                 |\n+----------------------------------+\n\nDistinct Count of Department & Salary: 8\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##count function\n\n\n**count() function returns number of elements in a column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"609cd9d7-b09e-4847-8589-d5940dccc948","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(\"count: \"+ str(df.select(count('salary')).collect()[0][0]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af4008b6-924e-4107-9d2b-ae40897a810b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"count: 10\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["count: 10\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##grouping function\n\n\n**grouping() Indicates whether a given input column is aggregated or not. returns 1 for aggregated or 0 for not aggregated in the result. If you try grouping directly on the salary column you will get below error.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d570e6d-7a1c-43ae-adc8-44d0f36f0aaa","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(grouping('salary')).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2329c22e-a640-4d34-8360-071aee5f4ce7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4059022665744782>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrouping\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msalary\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2259\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2235\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2236\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2237\u001B[0m \n\u001B[1;32m   2238\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2257\u001B[0m \u001B[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001B[39;00m\n\u001B[1;32m   2258\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2259\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:215\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    211\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 215\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: grouping() can only be used with GroupingSets/Cube/Rollup;\n!Project [grouping(salary#160L) AS grouping(salary)#7334]\n+- LogicalRDD [employee_name#158, department#159, Salary#160L], false\n","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: grouping() can only be used with GroupingSets/Cube/Rollup;\n!Project [grouping(salary#160L) AS grouping(salary)#7334]\n+- LogicalRDD [employee_name#158, department#159, Salary#160L], false\n","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4059022665744782>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrouping\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msalary\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2259\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2235\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2236\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2237\u001B[0m \n\u001B[1;32m   2238\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2257\u001B[0m \u001B[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001B[39;00m\n\u001B[1;32m   2258\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2259\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:215\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    211\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 215\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: grouping() can only be used with GroupingSets/Cube/Rollup;\n!Project [grouping(salary#160L) AS grouping(salary)#7334]\n+- LogicalRDD [employee_name#158, department#159, Salary#160L], false\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.cube(\"employee_name\").agg(grouping('employee_name'), sum(\"salary\")).orderBy(\"employee_name\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"76e7f8ba-9a4f-4e6f-b178-3621e6ac6fe1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+-----------------------+-----------+\n|employee_name|grouping(employee_name)|sum(salary)|\n+-------------+-----------------------+-----------+\n|null         |1                      |34000      |\n|James        |0                      |6000       |\n|Jeff         |0                      |3000       |\n|Jen          |0                      |3900       |\n|Kumar        |0                      |2000       |\n|Maria        |0                      |3000       |\n|Michael      |0                      |4600       |\n|Robert       |0                      |4100       |\n|Saif         |0                      |4100       |\n|Scott        |0                      |3300       |\n+-------------+-----------------------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+-----------------------+-----------+\n|employee_name|grouping(employee_name)|sum(salary)|\n+-------------+-----------------------+-----------+\n|null         |1                      |34000      |\n|James        |0                      |6000       |\n|Jeff         |0                      |3000       |\n|Jen          |0                      |3900       |\n|Kumar        |0                      |2000       |\n|Maria        |0                      |3000       |\n|Michael      |0                      |4600       |\n|Robert       |0                      |4100       |\n|Saif         |0                      |4100       |\n|Scott        |0                      |3300       |\n+-------------+-----------------------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##first function\n\n\n**first() function returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a52fd8f7-4856-44a8-abdf-688eec2012dc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#first\n\ndf.select(first(\"salary\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af22a228-dd87-40b0-babc-6d0090d987c5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+\n|first(salary)|\n+-------------+\n|3000         |\n+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+\n|first(salary)|\n+-------------+\n|3000         |\n+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##last function\n\n\n**last() function returns the last element in a column. when ignoreNulls is set to true, it returns the last non-null element.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4c491bf1-5be4-4c49-a949-3902b5327446","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#last\n\ndf.select(last('salary')).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9210fcfc-5b75-47bf-8bd2-02116a6e988a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------+\n|last(salary)|\n+------------+\n|4100        |\n+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------+\n|last(salary)|\n+------------+\n|4100        |\n+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##kurtosis function\n\n\n**kurtosis() function returns the kurtosis of the values in a group.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c316ec97-278e-411b-b790-ea7c0bc21442","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(kurtosis('salary')).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b6de34d-12bb-404b-9b11-3199eb3c99fc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------------+\n|kurtosis(salary)   |\n+-------------------+\n|-0.6467803030303032|\n+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------------+\n|kurtosis(salary)   |\n+-------------------+\n|-0.6467803030303032|\n+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##max function\n\n\n**max() function returns the maximum value in a column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e929a65-b7f0-4880-8684-1d6eb81ac593","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(max('salary')).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68040b81-5daf-4151-9ea5-6e3f7b9d3434","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+\n|max(salary)|\n+-----------+\n|4600       |\n+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+\n|max(salary)|\n+-----------+\n|4600       |\n+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##min function\n\n\n**min() function**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e6a3e8c-a1b4-4cc8-9451-93666fdb34ba","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(min('salary')).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d78b5d6-7b70-435e-9f44-02d47db65081","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+\n|min(salary)|\n+-----------+\n|2000       |\n+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+\n|min(salary)|\n+-----------+\n|2000       |\n+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##mean function\n\n\n**mean() function returns the average of the values in a column. Alias for Avg**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f20a094-2a4f-4fee-a7a0-d443a30f9861","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(mean(\"salary\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11ef6de1-a6a1-4c90-a84f-d0e5205d356e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+\n|avg(salary)|\n+-----------+\n|3400.0     |\n+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+\n|avg(salary)|\n+-----------+\n|3400.0     |\n+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##skewness function\n\n\n**skewness() function returns the skewness of the values in a group.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1e439ca-449a-4b02-ae58-7355e41f7fb6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(skewness(\"salary\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0abfd934-0b9e-4296-a084-458fcbc220b9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+\n|skewness(salary)    |\n+--------------------+\n|-0.12041791181069571|\n+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+\n|skewness(salary)    |\n+--------------------+\n|-0.12041791181069571|\n+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##stddev(), stddev_samp() and stddev_pop()\n\n- stddev() alias for stddev_samp.\n\n- stddev_samp() function returns the sample standard deviation of values in a column.\n\n- stddev_pop() function returns the population standard deviation of the values in a column."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10ef1ada-77fb-4153-8740-d20a02893562","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(stddev(\"salary\"), stddev_samp(\"salary\"), stddev_pop(\"salary\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a2775b0e-2c72-461b-b408-05782a8ab370","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------------+-------------------+------------------+\n|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n+-------------------+-------------------+------------------+\n|765.9416862050705  |765.9416862050705  |726.636084983398  |\n+-------------------+-------------------+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------------+-------------------+------------------+\n|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n+-------------------+-------------------+------------------+\n|765.9416862050705  |765.9416862050705  |726.636084983398  |\n+-------------------+-------------------+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##sum function\n\n\n**sum() function Returns the sum of all values in a column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"578301a3-4853-45d2-9814-6c89f8399b18","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(sum(\"salary\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b8004d7-b371-4e64-83f9-7995a1f96e28","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+\n|sum(salary)|\n+-----------+\n|34000      |\n+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+\n|sum(salary)|\n+-----------+\n|34000      |\n+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##sumDistinct function\n\n\n**sumDistinct() function returns the sum of all distinct values in a column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1c81d89-2b8a-4182-915b-ec48e779b3ee","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(sum_distinct(\"salary\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3901025c-a212-44ac-bd02-f09b8c80fd23","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|20900               |\n+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|20900               |\n+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##variance(), var_samp(), var_pop()\n\n\n- variance() alias for var_samp\n\n- var_samp() function returns the unbiased variance of the values in a column.\n\n- var_pop() function returns the population variance of the values in a column."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9efd26f1-d5f2-4f0d-8dc6-02d2735ba450","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(variance(\"salary\"), var_samp(\"salary\"), var_pop(\"salary\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c418481f-2c00-4367-beef-f9d9cdc2cde4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+-----------------+---------------+\n|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n+-----------------+-----------------+---------------+\n|586666.6666666666|586666.6666666666|528000.0       |\n+-----------------+-----------------+---------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-----------------+---------------+\n|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n+-----------------+-----------------+---------------+\n|586666.6666666666|586666.6666666666|528000.0       |\n+-----------------+-----------------+---------------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark Aggregate Functions with Examples","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4059022665744765}},"nbformat":4,"nbformat_minor":0}
