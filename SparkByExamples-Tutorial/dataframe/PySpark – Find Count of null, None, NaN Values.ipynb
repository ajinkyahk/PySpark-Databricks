{"cells":[{"cell_type":"markdown","source":["#PySpark – Find Count of null, None, NaN Values\n\n\n\n---\n\n\n\n**In PySpark DataFrame you can calculate the count of Null, None, NaN or Empty/Blank values in a column by using isNull() of Column class & SQL functions isnan() count() and when(). In this article, I will explain how to get the count of Null, None, NaN, empty or blank values from all or multiple selected columns of PySpark DataFrame.**\n\n\n---\n\n\n**Note: In Python None is equal to null value, soon on PySpark DataFrame None values are shown as null.**\n\n\n---\n\n**First let’s create a DataFrame with some Null, None, NaN & Empty/Blank values.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69dfda60-965c-4989-86b0-97db626ee18b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import numpy as np\n\ndata = [\n    (\"James\",\"CA\",np.NaN), (\"Julia\",\"\",None),\n    (\"Ram\",None,200.0), (\"Ramya\",\"NULL\",np.NAN)\n]\n\ndf = spark.createDataFrame(data, [\"name\", \"state\", \"number\"])\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25b694e5-108e-4928-94b0-119b94bd5dcf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+-----+------+\n|name |state|number|\n+-----+-----+------+\n|James|CA   |NaN   |\n|Julia|     |null  |\n|Ram  |null |200.0 |\n|Ramya|NULL |NaN   |\n+-----+-----+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+-----+------+\n|name |state|number|\n+-----+-----+------+\n|James|CA   |NaN   |\n|Julia|     |null  |\n|Ram  |null |200.0 |\n|Ramya|NULL |NaN   |\n+-----+-----+------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Find Count of Null, None, NaN of All DataFrame Columns\n\n\n**df.columns returns all DataFrame columns as a list, will loop through the list, and check each column has Null or NaN values. In the below snippet isnan() is a SQL function that is used to check for NAN values and isNull() is a Column class function that is used to check for Null values.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"610ba686-d6d8-44b1-96bf-64502ae601f1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Find Count of Null , None, NaN of All  DataFrame Columns\n\nfrom pyspark.sql.functions import col, isnan, when, count\n\ndf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1577382-00cb-42fa-ba07-1503d8f05ce2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----+-----+------+\n|name|state|number|\n+----+-----+------+\n|0   |1    |3     |\n+----+-----+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----+-----+------+\n|name|state|number|\n+----+-----+------+\n|0   |1    |3     |\n+----+-----+------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Note: This example doesn’t count columns containing NULL string literal values, I will cover this in the next section so keep reading.**\n\n**To find count for selected columns in a list use list of column names instead of df.columns. The below example yields the same output as above.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e2cb8dc-dafb-4b36-9d63-94db52507830","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Find count for selected columns\n\ndf_columns = [\"name\", \"state\", \"number\"]\n\ndf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_columns]).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f940e13-9ddd-4e49-a86e-096e70c19b8b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----+-----+------+\n|name|state|number|\n+----+-----+------+\n|0   |1    |3     |\n+----+-----+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----+-----+------+\n|name|state|number|\n+----+-----+------+\n|0   |1    |3     |\n+----+-----+------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Find Count of None, NULL & Empty String Literal Values\n\n**In case if you have “Null”, “None”, and empty string literal values, use contains() of PySpark Column class**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ccbfe905-5249-4dd9-a63e-ede962ddd907","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Find count for empty, None, Null, NaN with string literals.\n\ndf2 = df.select([count(when(col(c).contains('None')|\\\n                           col(c).contains('NULL')|\\\n                           (col(c)== '')|\\\n                           col(c).isNull()|\\\n                           isnan(c),c)).alias(c)  for c in df.columns])\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7441f1a2-8ba8-40fc-94df-816e4156fa88","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----+-----+------+\n|name|state|number|\n+----+-----+------+\n|0   |3    |3     |\n+----+-----+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----+-----+------+\n|name|state|number|\n+----+-----+------+\n|0   |3    |3     |\n+----+-----+------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark – Find Count of null, None, NaN Values","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2450113531380184}},"nbformat":4,"nbformat_minor":0}
