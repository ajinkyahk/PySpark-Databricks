{"cells":[{"cell_type":"markdown","source":["#PySpark MapType (Dict) Usage with Examples\n\n\n---\n\n**PySpark MapType (also called map type) is a data type to represent Python Dictionary (dict) to store key-value pair, a MapType object comprises three fields, keyType (a DataType), valueType (a DataType) and valueContainsNull (a BooleanType).**\n\n\n###What is PySpark MapType\n\n\n**PySpark MapType is used to represent map key-value pair similar to python Dictionary (Dict), it extends DataType class which is a superclass of all types in PySpark and takes two mandatory arguments keyType and valueType of type DataType and one optional boolean argument valueContainsNull. keyType and valueType can be any type that extends the DataType class. for e.g StringType, IntegerType, ArrayType, MapType, StructType (struct) e.t.c.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e87e3278-3bed-4173-8dcd-71d15191493b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##1. Create PySpark MapType\n\n\n**In order to use MapType data type first, you need to import it from pyspark.sql.types.MapType and use MapType() constructor to create a map object.**\n\n\n###from pyspark.sql.types import StringType, MapType\n###mapCol = MapType(StringType(),StringType(),False)\n\n---\n\n\n##MapType Key Points:\n\n\n- The First param keyType is used to specify the type of the key in the map.\n\n- The Second param valueType is used to specify the type of the value in the map.\n\n- Third parm valueContainsNull is an optional boolean type that is used to specify if the value of the second param can accept Null/None values.\n\n- The key of the map won’t accept None/Null values.\n\n- PySpark provides several SQL functions to work with MapType.\n\n\n---\n\n\n##2. Create MapType From StructType\n\n\n**Let’s see how to create a MapType by using PySpark StructType & StructField, StructType() constructor takes list of StructField, StructField takes a fieldname and type of the value.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"910979eb-dac2-4792-942f-37e6f4a998d5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, StringType, MapType\n\nschema = StructType([\n    StructField('name', StringType(),True),\n    StructField('properties', MapType(StringType(), StringType()), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa4997ec-d4d0-4245-80dc-6f97e8377ba9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dataDictionary = [\n    ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n]\n\ndf = spark.createDataFrame(data=dataDictionary, schema=schema)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"95b08ca4-e880-44c1-baf8-f3cacf213519","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##3. Access PySpark MapType Elements\n\n\n**Let’s see how to extract the key and values from the PySpark DataFrame Dictionary column. Here I have used PySpark map transformation to read the values of properties (MapType column)**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"00f624a7-649c-46e0-b021-900a4b19827b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df2 = df.rdd.map(lambda x:\\\n(x.name, x.properties['hair'], x.properties['eye']))\\\n.toDF(['name', 'hair', 'eye'])\n\ndf2.printSchema()\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"38bcb620-9418-43ac-9203-48a2638ee1ab","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|name      |hair |eye  |\n+----------+-----+-----+\n|James     |black|brown|\n|Michael   |brown|null |\n|Robert    |red  |black|\n|Washington|grey |grey |\n|Jefferson |brown|     |\n+----------+-----+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|name      |hair |eye  |\n+----------+-----+-----+\n|James     |black|brown|\n|Michael   |brown|null |\n|Robert    |red  |black|\n|Washington|grey |grey |\n|Jefferson |brown|     |\n+----------+-----+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Let’s use another way to get the value of a key from Map using getItem() of Column type, this method takes a key as an argument and returns a value.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"429ac758-b95a-4db9-9d77-dc4765f5d34c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.withColumn('hair', df.properties.getItem('hair'))\\\n.withColumn('eye', df.properties.getItem('eye'))\\\n.drop('properties')\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35d7e603-d626-4e30-8378-1a5a28f944e8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+-----+-----+\n|name      |hair |eye  |\n+----------+-----+-----+\n|James     |black|brown|\n|Michael   |brown|null |\n|Robert    |red  |black|\n|Washington|grey |grey |\n|Jefferson |brown|     |\n+----------+-----+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+-----+-----+\n|name      |hair |eye  |\n+----------+-----+-----+\n|James     |black|brown|\n|Michael   |brown|null |\n|Robert    |red  |black|\n|Washington|grey |grey |\n|Jefferson |brown|     |\n+----------+-----+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##4. Functions\n\n**Below are some of the MapType Functions with examples.**\n\n###4.1 – explode"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5cad53d6-fc2c-47d6-8240-0df37e12fcef","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n\ndf.select(df.name, explode(df.properties)).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c9f109a1-8f77-42be-bf38-03949802b7ac","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+----+-----+\n|name      |key |value|\n+----------+----+-----+\n|James     |eye |brown|\n|James     |hair|black|\n|Michael   |eye |null |\n|Michael   |hair|brown|\n|Robert    |eye |black|\n|Robert    |hair|red  |\n|Washington|eye |grey |\n|Washington|hair|grey |\n|Jefferson |eye |     |\n|Jefferson |hair|brown|\n+----------+----+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+----+-----+\n|name      |key |value|\n+----------+----+-----+\n|James     |eye |brown|\n|James     |hair|black|\n|Michael   |eye |null |\n|Michael   |hair|brown|\n|Robert    |eye |black|\n|Robert    |hair|red  |\n|Washington|eye |grey |\n|Washington|hair|grey |\n|Jefferson |eye |     |\n|Jefferson |hair|brown|\n+----------+----+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###4.2 map_keys() – Get All Map Keys"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6bcde3f6-1d0c-444a-ad25-6769942eb4f4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import map_keys\n\ndf.select(df.name, map_keys(df.properties)).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d8b08202-ca4b-4a4d-9a25-e8b037898703","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+--------------------+\n|name      |map_keys(properties)|\n+----------+--------------------+\n|James     |[eye, hair]         |\n|Michael   |[eye, hair]         |\n|Robert    |[eye, hair]         |\n|Washington|[eye, hair]         |\n|Jefferson |[eye, hair]         |\n+----------+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+--------------------+\n|name      |map_keys(properties)|\n+----------+--------------------+\n|James     |[eye, hair]         |\n|Michael   |[eye, hair]         |\n|Robert    |[eye, hair]         |\n|Washington|[eye, hair]         |\n|Jefferson |[eye, hair]         |\n+----------+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**In case if you wanted to get all map keys as Python List. WARNING: This runs very slow.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"275df744-9bed-4535-9a88-baac0f5eb848","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import explode, map_keys\n\nkeysDF = df.select(explode(map_keys(df.properties))).distinct()\nkeysDF.show()\nkeysList = keysDF.rdd.map(lambda x: x[0]).collect()\nprint(keysList)\n#['eye', 'hair']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3f5f7190-b4c8-4e4e-8002-247a8d2f637d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----+\n| col|\n+----+\n| eye|\n|hair|\n+----+\n\n['eye', 'hair']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----+\n| col|\n+----+\n| eye|\n|hair|\n+----+\n\n['eye', 'hair']\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###4.3 map_values() – Get All map Values"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e306ddd-29b8-41a0-8ec9-dd53246951a3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import map_values\n\ndf.select(df.name, map_values(df.properties)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"749c88b2-070c-48c0-94dc-db7f68c46803","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+----------------------+\n|      name|map_values(properties)|\n+----------+----------------------+\n|     James|        [brown, black]|\n|   Michael|         [null, brown]|\n|    Robert|          [black, red]|\n|Washington|          [grey, grey]|\n| Jefferson|             [, brown]|\n+----------+----------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+----------------------+\n|      name|map_values(properties)|\n+----------+----------------------+\n|     James|        [brown, black]|\n|   Michael|         [null, brown]|\n|    Robert|          [black, red]|\n|Washington|          [grey, grey]|\n| Jefferson|             [, brown]|\n+----------+----------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###Conclusion\n\n\n**MapType is a map data structure that is used to store key key-value pairs similar to Python Dictionary (Dic), keys and values type of map should be of a type that extends DataType. Key won’t accept null/None values whereas map of the key can have None/Null value.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69393172-4fda-4637-ade5-1dfeeecc8bb4","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark MapType (Dict) Usage with Examples","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1110408793256919}},"nbformat":4,"nbformat_minor":0}
