{"cells":[{"cell_type":"markdown","source":["#PySpark foreach() Usage with Examples\n\n\n---\n\n\n**PySpark foreach() is an action operation that is available in RDD, DataFram to iterate/loop over each element in the DataFrmae, It is similar to for with advanced concepts. This is different than other actions as foreach() function doesn’t return a value instead it executes the input function on each element of an RDD, DataFrame**\n\n\n\n---\n\n\n\n##1. PySpark DataFrame foreach()\n\n\n---\n\n\n\n###1.1 foreach() Syntax\n\n\n\n**Following is the syntax of the foreach() function**\n\n### Syntax\n####DataFrame.foreach(f)\n\n\n---\n\n\n###1.2 PySpark foreach() Usage\n\n\n**When foreach() applied on PySpark DataFrame, it executes a function specified in for each element of DataFrame. This operation is mainly used if you wanted to manipulate accumulators, save the DataFrame results to RDBMS tables, Kafka topics, and other external sources.**\n\n---\n\n**In this example, to make it simple we just print the DataFrame to the console.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"20a770ff-8756-485c-aee9-abb82d563814","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Prepare Data \n\ncolumns = [\"Seqno\", \"Name\"]\n\ndata = [\n    (\"1\", \"john jones\"),\n    (\"2\", \"tracey smith\"),\n    (\"3\", \"amy sanders\")\n]\n\ndf = spark.createDataFrame(data=data, schema=columns)\ndf.show(truncate=False)\n\n\n#foreach() Example\n\ndef func(df):\n    print(df.Seqno)\n    \ndf.rdd.foreach(func)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"171daeab-6d39-4fce-ac61-ea21a9f2fc65","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Here df.Seqno is printed in the worker nodes so you would not see any output in the driver program**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5600e265-05c9-4e7d-acb0-6a9f8a6876bf","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["**Using foreach() to update the accumulator shared variable.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"661569f2-292d-4566-b4b4-372d2e6f73ac","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# foreach() with accumulator Example\n\naccum = sc.accumulator(0)\n\ndf.foreach(lambda x:accum.add(int(x.Seqno)))\nprint(accum.value)      #Accessed by driver"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ad5e02a8-462e-40d0-8436-9c09236bf3c0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"6\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["6\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##2. PySpark RDD foreach() Usage\n\n\n---\n\n\n**The foreach() on RDD behaves similarly to DataFrame equivalent, hence the same syntax and it is also used to manipulate accumulators from RDD, and write external data sources.**\n\n---\n\n###2.1 Syntax\n\n### Syntax\n##RDD.foreach(f: Callable[[T], None]) → None\n\n\n---\n\n\n###2.2 RDD foreach() Example"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9166e530-a377-4822-941c-ece26a2230ac","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# foreach() with RDD example\n\naccum = sc.accumulator(0)\n\nrdd = sc.parallelize([1,2,3,4,5])\n\nrdd.foreach(lambda x: accum.add(x))\n\nprint(accum.value) #Accessed by driver"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f244845d-c6fb-45ea-b134-0e93ace34cf2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"15\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["15\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###Conclusion\n\n**In conclusion, PySpark foreach() is an action operation of RDD and DataFrame which doesn’t have any return type and is used to manipulate the accumulator and write any external data sources.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d2313e7-83cc-4a53-b8a5-3c93ebbf0e01","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark foreach() Usage with Examples","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4425648766185686}},"nbformat":4,"nbformat_minor":0}
