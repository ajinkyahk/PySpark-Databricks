{"cells":[{"cell_type":"markdown","source":["#PySpark transform() Function with Example\n\n---\n\n**PySpark provides two transform() functions one with DataFrame and another in pyspark.sql.functions.**\n\n\n- pyspark.sql.DataFrame.transform() – Available since Spark 3.0\n\n- pyspark.sql.functions.transform()\n\n---\n\n***In this article, I will explain the syntax of these two functions and explain with examples.***\n\n---\n\n**First, let’s create the DataFrame.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c790bf5c-6bb2-4db9-b454-613bab97c95b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Prepare Data\n\nsimpleData = [\n    (\"Java\",4000,5), \\\n    (\"Python\", 4600,10),  \\\n    (\"Scala\", 4100,15),   \\\n    (\"Scala\", 4500,15),   \\\n    (\"PHP\", 3000,20),  \\\n]\n\ncolumns = [\"CourseName\", \"fee\", \"discount\"]\n\ndf = spark.createDataFrame(data=simpleData, schema=columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ee3d93bc-6773-46a4-b352-849093992ad8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- CourseName: string (nullable = true)\n |-- fee: long (nullable = true)\n |-- discount: long (nullable = true)\n\n+----------+----+--------+\n|CourseName|fee |discount|\n+----------+----+--------+\n|Java      |4000|5       |\n|Python    |4600|10      |\n|Scala     |4100|15      |\n|Scala     |4500|15      |\n|PHP       |3000|20      |\n+----------+----+--------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- CourseName: string (nullable = true)\n |-- fee: long (nullable = true)\n |-- discount: long (nullable = true)\n\n+----------+----+--------+\n|CourseName|fee |discount|\n+----------+----+--------+\n|Java      |4000|5       |\n|Python    |4600|10      |\n|Scala     |4100|15      |\n|Scala     |4500|15      |\n|PHP       |3000|20      |\n+----------+----+--------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##1. PySpark DataFrame.transform()\n\n\n---\n\n\n**The pyspark.sql.DataFrame.transform() is used to chain the custom transformations and this function returns the new DataFrame after applying the specified transformations.**\n\n**This function always returns the same number of rows that exists on the input PySpark DataFrame.**\n\n\n---\n\n###1.1 Syntax\n\n---\n\n**Following is the syntax of the pyspark.sql.DataFrame.transform() function**\n\n\n### Syntax\n####DataFrame.transform(func: Callable[[…], DataFrame], *args: Any, **kwargs: Any) → pyspark.sql.dataframe.DataFrame\n\n\n---\n\n**The following are the parameters:**\n\n- func – Custom function to call.\n- *args – Arguments to pass to func.\n- *kwargs – Keyword arguments to pass to func."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e429e0b-2943-4fb1-9361-3f1d6e723cdd","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###1.2 Create Custom Functions\n\n\n---\n\n\n**In the below snippet, I have created the three custom transformations to be applied to the DataFrame. These transformations are nothing but Python functions that take the DataFrame apply some changes and return the new DataFrame.**\n\n- to_upper_str_columns() – This function converts the CourseName column to upper case and updates the same column.\n\n- reduce_price() – This function takes the argument and reduces the value from the fee and creates a new column.\n\n- apply_discount() – This creates a new column with the discounted fee."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"05383216-da3c-4022-8dab-31e071f2ccd1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Custom tranformation 1\n\nfrom pyspark.sql.functions import upper\n\ndef to_upper_str_columns(df):\n    return df.withColumn(\"CourseName\", upper(df.CourseName))\n\n\n# Custom Transformation 2\n\ndef reduce_price(df, reduceBy):\n    return df.withColumn(\"new_fee\", df.fee - reduceBy)\n\n\n# Custom Transformation 3\n\ndef apply_discount(df):\n    return df.withColumn(\"discounted_fee\",\\\n                         df.new_fee -(df.new_fee * df.discount)/100)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"79beb7fc-c16d-4cae-b49e-4dfbf5895bf7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###1.3 PySpark Apply DataFrame.transform()\n\n---\n\n\n**Now, let’s chain these custom functions together and run them using PySpark DataFrame transform() function.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"854c295d-e0ca-4ecc-a807-3671a33b4554","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# PySpark transform() Usage \n\ndf2 = df.transform(to_upper_str_columns)\\\n.transform(reduce_price, 1000)\\\n.transform(apply_discount)\n\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"56458a90-0048-4b56-a696-03889356db30","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+----+--------+-------+--------------+\n|CourseName|fee |discount|new_fee|discounted_fee|\n+----------+----+--------+-------+--------------+\n|JAVA      |4000|5       |3000   |2850.0        |\n|PYTHON    |4600|10      |3600   |3240.0        |\n|SCALA     |4100|15      |3100   |2635.0        |\n|SCALA     |4500|15      |3500   |2975.0        |\n|PHP       |3000|20      |2000   |1600.0        |\n+----------+----+--------+-------+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+----+--------+-------+--------------+\n|CourseName|fee |discount|new_fee|discounted_fee|\n+----------+----+--------+-------+--------------+\n|JAVA      |4000|5       |3000   |2850.0        |\n|PYTHON    |4600|10      |3600   |3240.0        |\n|SCALA     |4100|15      |3100   |2635.0        |\n|SCALA     |4500|15      |3500   |2975.0        |\n|PHP       |3000|20      |2000   |1600.0        |\n+----------+----+--------+-------+--------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**In case you wanted to select the columns either you can chain it with select() or create another custom function.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a37ace0a-1958-41d5-b1a0-f4622b97294e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Custom Fuction\n\ndef select_columns(df):\n    return df.select(\"CourseName\", \"discounted_fee\")\n\n\n#Chain Transformation\n\ndf2 = df.transform(to_upper_str_columns)\\\n.transform(reduce_price, 1000)\\\n.transform(apply_discount)\\\n.transform(select_columns)\n\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"71e40027-9265-487b-81a0-af9df1c6ceb4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+--------------+\n|CourseName|discounted_fee|\n+----------+--------------+\n|JAVA      |2850.0        |\n|PYTHON    |3240.0        |\n|SCALA     |2635.0        |\n|SCALA     |2975.0        |\n|PHP       |1600.0        |\n+----------+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+--------------+\n|CourseName|discounted_fee|\n+----------+--------------+\n|JAVA      |2850.0        |\n|PYTHON    |3240.0        |\n|SCALA     |2635.0        |\n|SCALA     |2975.0        |\n|PHP       |1600.0        |\n+----------+--------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##2. PySpark sql.functions.transform()\n\n\n---\n\n\n**The PySpark sql.functions.transform() is used to apply the transformation on a column of type Array. This function applies the specified transformation on every element of the array and returns an object of ArrayType.**\n\n\n---\n\n###2.1 Syntax\n\n**Following is the syntax of the pyspark.sql.functions.transform() function**\n\n\n### Syntax\n#pyspark.sql.functions.transform(col, f)\n\n\n---\n\n\n**The following are the parameters:**\n\n- col – ArrayType column\n\n- f – Optional. Function to apply.\n\n\n---\n\n\n###2.2 Example\n\n**Since our above DataFrame doesn’t contain ArrayType, I will create a new simple array to explain.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"422f81e1-9e93-42d4-9eac-feae7c498139","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Create DataFrame with Array\n\ndata = [\n (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n]\n\ndf = spark.createDataFrame(data=data, schema=[\"Name\", \"Languages1\", \"Languages2\"])\ndf.printSchema()\ndf.show(truncate=False)\n\n\n# using transform() function\n\nfrom pyspark.sql.functions import upper\nfrom pyspark.sql.functions import transform\n\ndf.select(transform(\"Languages1\", lambda x: upper(x)).alias(\"languages1\"))\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec76a5dd-e298-413b-8c56-c599afddb27d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Name: string (nullable = true)\n |-- Languages1: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- Languages2: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n+----------------+------------------+---------------+\n|Name            |Languages1        |Languages2     |\n+----------------+------------------+---------------+\n|James,,Smith    |[Java, Scala, C++]|[Spark, Java]  |\n|Michael,Rose,   |[Spark, Java, C++]|[Spark, Java]  |\n|Robert,,Williams|[CSharp, VB]      |[Spark, Python]|\n+----------------+------------------+---------------+\n\n+------------------+\n|languages1        |\n+------------------+\n|[JAVA, SCALA, C++]|\n|[SPARK, JAVA, C++]|\n|[CSHARP, VB]      |\n+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Name: string (nullable = true)\n |-- Languages1: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- Languages2: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n+----------------+------------------+---------------+\n|Name            |Languages1        |Languages2     |\n+----------------+------------------+---------------+\n|James,,Smith    |[Java, Scala, C++]|[Spark, Java]  |\n|Michael,Rose,   |[Spark, Java, C++]|[Spark, Java]  |\n|Robert,,Williams|[CSharp, VB]      |[Spark, Python]|\n+----------------+------------------+---------------+\n\n+------------------+\n|languages1        |\n+------------------+\n|[JAVA, SCALA, C++]|\n|[SPARK, JAVA, C++]|\n|[CSHARP, VB]      |\n+------------------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark transform() Function with Example","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4154264259724654}},"nbformat":4,"nbformat_minor":0}
