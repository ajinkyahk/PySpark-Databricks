{"cells":[{"cell_type":"markdown","source":["# Convert PySpark RDD to DataFrame\n\n---\n\n**In PySpark, toDF() function of the RDD is used to convert RDD to DataFrame. We would need to convert RDD to DataFrame as DataFrame provides more advantages over RDD. For instance, DataFrame is a distributed collection of data organized into named columns similar to Database tables and provides optimization and performance improvements.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a9301abd-d193-44f0-a79c-fe9d28d79b0b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## 1. Create PySpark RDD\n\n--- \n\n**In PySpark, when you have data in a list meaning you have a collection of data in a PySpark driver memory when you create an RDD, this collection is going to be parallelized.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d3b87652-f827-4cd5-83ac-33f00937e4d7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["\ndept = [('Finance', 10), ('Marketing', 20), ('Sales', 30), ('IT', 40)]\n\nrdd = sc.parallelize(dept)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e81d5b65-c375-4bd8-88e6-d8a4b43ce610","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##2. Convert PySpark RDD to DataFrame\n\n---\n\n**Converting PySpark RDD to DataFrame can be done using toDF(), createDataFrame(). In this section**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2161852c-6378-4c3b-82b2-55d46bcf3b47","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###2.1 Using rdd.toDF() function\n\n---\n\n**PySpark provides toDF() function in RDD which can be used to convert RDD into Dataframe**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0242d939-8131-443d-bee8-c42b36d50ea4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = rdd.toDF()\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d71401f0-60b0-4e2a-a6e0-09ab3c6f71a3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+---------+---+\n|_1       |_2 |\n+---------+---+\n|Finance  |10 |\n|Marketing|20 |\n|Sales    |30 |\n|IT       |40 |\n+---------+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+---------+---+\n|_1       |_2 |\n+---------+---+\n|Finance  |10 |\n|Marketing|20 |\n|Sales    |30 |\n|IT       |40 |\n+---------+---+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**toDF() has another signature that takes arguments to define column names as shown below.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"67d8d134-eaf2-4e5e-ab39-25b3a1a17ef3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["deptColumns = ['dept_name', 'dept_id']\n\ndf2 = rdd.toDF(schema=deptColumns)\ndf2.printSchema()\n\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ea44d95c-438a-4c23-b9aa-b28fbff842c4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 2.2 Using PySpark createDataFrame() function\n\n---\n\n**sparkSession class provides createDataFrame() method to create DataFrame and it takes rdd object as an argument.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d14ec6dd-dcc9-44a9-8366-8118bb0429a9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["deptDF = spark.createDataFrame(data=rdd, schema=deptColumns)\n\ndeptDF.printSchema()\n\ndeptDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"372200b3-9de5-4e36-a67b-fa81085b3c9f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###2.3 Using createDataFrame() with StructType schema\n\n---\n\n\n**When you infer the schema, by default the datatype of the columns is derived from the data and set’s nullable to true for all columns. We can change this behavior by supplying schema using StructType – where we can specify a column name, data type and nullable for each field/column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9717a2af-395f-4bbf-968f-8f674af49b30","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType\n\ndeptSchema = StructType([\n    StructField('dept_name', StringType(), True),\n    StructField('dept_id', StringType(), True)\n])\n\n\ndeptDF1 = spark.createDataFrame(data=rdd, schema=deptSchema)\n\ndeptDF1.printSchema()\n\ndeptDF1.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"06857d00-d24b-4628-a43b-a5d921e59b13","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- dept_id: string (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: string (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Convert PySpark RDD to DataFrame","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1460591441327617}},"nbformat":4,"nbformat_minor":0}
