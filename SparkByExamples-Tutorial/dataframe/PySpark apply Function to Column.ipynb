{"cells":[{"cell_type":"markdown","source":["#PySpark apply Function to Column\n\n\n---\n\n**How to apply a function to a column in PySpark? By using withColumn(), sql(), select() you can apply a built-in function or custom function to a column. In order to apply a custom function, first you need to create a function and register the function as a UDF. Recent versions of PySpark provide a way to use Pandas API hence, you can also use pyspark.pandas.DataFrame.apply().**\n\n---\n\n**Let’s create a PySpark DataFrame.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51c2b9df-fff5-4ce8-b556-763caf88b126","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["columns = [\"Seqno\", \"Name\"]\n\ndata = [\n    (\"1\", \"john jones\"),\n    (\"2\", \"tracey smith\"),\n    (\"3\", \"amy sanders\")\n]\n\ndf = spark.createDataFrame(data = data, schema=columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8148b231-3e71-453b-bf4a-6f5cf8e44396","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Seqno: string (nullable = true)\n |-- Name: string (nullable = true)\n\n+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Seqno: string (nullable = true)\n |-- Name: string (nullable = true)\n\n+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##1. PySpark apply Function using withColumn()\n\n\n---\n\n\n**PySpark withColumn() is a transformation function that is used to apply a function to the column. The below example applies an upper() function to column df.Name.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49b9bac4-12b7-462f-870a-2ae2db992cdb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Apply function using withColumn\n\nfrom pyspark.sql.functions import upper\n\ndf.withColumn(\"Upper_Name\", upper(df.Name))\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2b5c0720-9961-4c4e-9e31-21b493576bcf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------------+------------+\n|Seqno|Name        |Upper_Name  |\n+-----+------------+------------+\n|1    |john jones  |JOHN JONES  |\n|2    |tracey smith|TRACEY SMITH|\n|3    |amy sanders |AMY SANDERS |\n+-----+------------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------------+------------+\n|Seqno|Name        |Upper_Name  |\n+-----+------------+------------+\n|1    |john jones  |JOHN JONES  |\n|2    |tracey smith|TRACEY SMITH|\n|3    |amy sanders |AMY SANDERS |\n+-----+------------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##2. Apply Function using select()\n\n\n---\n\n\n**The select() is used to select the columns from the PySpark DataFrame while selecting the columns you can also apply the function to a column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ec1c497-ba76-4ee4-853f-1b6073bf36d3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Apply function using select\n\ndf.select(\"Seqno\", \"Name\", upper(df.Name))\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"17914451-9d0d-4c80-86b6-bdfdc737e31a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------------+------------+\n|Seqno|Name        |upper(Name) |\n+-----+------------+------------+\n|1    |john jones  |JOHN JONES  |\n|2    |tracey smith|TRACEY SMITH|\n|3    |amy sanders |AMY SANDERS |\n+-----+------------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------------+------------+\n|Seqno|Name        |upper(Name) |\n+-----+------------+------------+\n|1    |john jones  |JOHN JONES  |\n|2    |tracey smith|TRACEY SMITH|\n|3    |amy sanders |AMY SANDERS |\n+-----+------------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##3. Apply Function using sql()\n\n\n---\n\n\n**You can also apply the function to the column while running the SQL query on the PySpark DataFrame. In order to use SQL, make sure you create a temporary view using createOrReplaceTempView().**\n\n**To run the SQL query use spark.sql() function and create the table by using createOrReplaceTempView(). This table would be available to use until you end your current SparkSession.**\n\n**spark.sql() returns a DataFrame and here, I have used show() to display the contents to console.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eef980fa-8b2b-4a8c-8027-c33d74d06237","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Apply function using sql()\n\ndf.createOrReplaceTempView(\"TAB\")\n\nspark.sql(\" select Seqno, Name, UPPER(Name) from TAB\")\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"201737d0-afa7-4de3-8ac2-36b11797aaf4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------------+------------+\n|Seqno|Name        |upper(Name) |\n+-----+------------+------------+\n|1    |john jones  |JOHN JONES  |\n|2    |tracey smith|TRACEY SMITH|\n|3    |amy sanders |AMY SANDERS |\n+-----+------------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------------+------------+\n|Seqno|Name        |upper(Name) |\n+-----+------------+------------+\n|1    |john jones  |JOHN JONES  |\n|2    |tracey smith|TRACEY SMITH|\n|3    |amy sanders |AMY SANDERS |\n+-----+------------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##4. PySpark apply Custom UDF Function\n\n\n---\n\n\n**In this section, I will explain how to create a custom PySpark UDF function and apply this function to a column.**\n\n**PySpark UDF (a.k.a User Defined Function) is the most useful feature of Spark SQL & DataFrame that is used to extend the PySpark built-in capabilities. Note that UDFs are the most expensive operations hence use them only if you have no choice and when essential.**\n\n\n---\n\n\n**Following are the steps to apply a custom UDF function on an SQL query.**\n\n\n###4.1 Create Custom Function\n\n**First, create a python function. Though upper() is already available in the PySpark SQL function, to make the example simple, I would like to create one.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"443d2ebb-15c9-458d-912d-bc7e667b3b3a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Create custom function\ndef upperCase(name_str):\n    return name_str.upper()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"21ffab93-58af-4d98-b66a-69d2cc2d94d6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###4.2 Register UDF\n\n\n**Create a udf function by wrapping the above function with udf().**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"256fe68a-5212-44f9-93be-e60e6106884f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#convert fuction to udf\n\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import StringType\n\nupperCaseUDF = udf(lambda x: upperCase(x), StringType())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c835e4b1-4735-4f6b-932f-cb76e2d5df75","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###4.3 Apply Custom UDF to Column\n\n\n**Finally apply the function to the column by using withColumn(), select() and sql().**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"78234429-1f61-4d01-9258-106d3fc55fd5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Custom udf with withColumn()\n\ndf.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\")))\\\n.show(truncate=False)\n\n\n# Custom function with select()\n\ndf.select(col(\"Seqno\"),\\\n         upperCaseUDF(col(\"Name\")).alias(\"Name\"))\\\n.show(truncate=False)\n\n\n\n# Custom function with sql()\n\nspark.udf.register(\"upperCaseUDF\", upperCaseUDF)\ndf.createOrReplaceTempView(\"TAB\")\n\nspark.sql(\" select Seqno, Name, upperCaseUDF(Name) from TAB \")\\\n.show(truncate=False)\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c901331f-0b9a-470f-8e4c-7dec26f9e87b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------------+-------------+\n|Seqno|Name        |Cureated Name|\n+-----+------------+-------------+\n|1    |john jones  |JOHN JONES   |\n|2    |tracey smith|TRACEY SMITH |\n|3    |amy sanders |AMY SANDERS  |\n+-----+------------+-------------+\n\n+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |JOHN JONES  |\n|2    |TRACEY SMITH|\n|3    |AMY SANDERS |\n+-----+------------+\n\n+-----+------------+------------------+\n|Seqno|Name        |upperCaseUDF(Name)|\n+-----+------------+------------------+\n|1    |john jones  |JOHN JONES        |\n|2    |tracey smith|TRACEY SMITH      |\n|3    |amy sanders |AMY SANDERS       |\n+-----+------------+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------------+-------------+\n|Seqno|Name        |Cureated Name|\n+-----+------------+-------------+\n|1    |john jones  |JOHN JONES   |\n|2    |tracey smith|TRACEY SMITH |\n|3    |amy sanders |AMY SANDERS  |\n+-----+------------+-------------+\n\n+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |JOHN JONES  |\n|2    |TRACEY SMITH|\n|3    |AMY SANDERS |\n+-----+------------+\n\n+-----+------------+------------------+\n|Seqno|Name        |upperCaseUDF(Name)|\n+-----+------------+------------------+\n|1    |john jones  |JOHN JONES        |\n|2    |tracey smith|TRACEY SMITH      |\n|3    |amy sanders |AMY SANDERS       |\n+-----+------------+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##5. PySpark Pandas apply()\n\n\n**PySpark DataFrame doesn’t contain the apply() function however, we can leverage Pandas DataFrame.apply() by running Pandas API over PySpark. Below is a simple example to give you an idea.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d976d2d-6b45-45e5-b9c1-83bf5a2726e7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Imports\n\nimport pyspark.pandas as ps\nimport numpy as np\n\ntechnologies = ({\n    'Fee' : [20000, 25000, 30000, 22000, np.NaN],\n    'Discount' : [1000, 2500, 1500, 1200, 3000]\n})\n\n\n#Create DataFrame \n\npsdf = ps.DataFrame(data=technologies)\nprint(psdf)\nprint(f'\\n\\n')\n\ndef add(data):\n    return data[0] + data[1]\n\naddDF = psdf.apply(add, axis=1)\nprint(addDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"559ed1ee-b648-4525-807f-decf7449ce3a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"       Fee  Discount\n0  20000.0      1000\n1  25000.0      2500\n2  30000.0      1500\n3  22000.0      1200\n4      NaN      3000\n\n\n\n0    21000.0\n1    27500.0\n2    31500.0\n3    23200.0\n4        NaN\ndtype: float64\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["       Fee  Discount\n0  20000.0      1000\n1  25000.0      2500\n2  30000.0      1500\n3  22000.0      1200\n4      NaN      3000\n\n\n\n0    21000.0\n1    27500.0\n2    31500.0\n3    23200.0\n4        NaN\ndtype: float64\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark apply Function to Column","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4154264259724667}},"nbformat":4,"nbformat_minor":0}
