{"cells":[{"cell_type":"markdown","source":["#PySpark Row using on DataFrame and RDD"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"efa79e26-2078-4ca8-af27-eaecddd45de3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["**In PySpark Row class is available by importing pyspark.sql.Row which is represented as a record/row in DataFrame, one can create a Row object by using named arguments, or create a custom Row like class.**\n\n---\n\n\n***how to use Row class on RDD, DataFrame and its functions.***"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"06f71c4a-c845-48d0-a9d1-02289be890a9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["####Key Points of Row Class:\n\n\n- Earlier to Spark 3.0, when used Row class with named arguments, the fields are sorted by name.\n- Since 3.0, Rows created from named arguments are not sorted alphabetically instead they will be ordered in the position entered.\n- To enable sorting by names, set the environment variable PYSPARK_ROW_FIELD_SORTING_ENABLED to true.\n- Row class provides a way to create a struct-type column as well."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dca18920-aa07-4108-a9ed-6bb8dc232164","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##1. Create a Row Object\n\n---\n\n**Row class extends the tuple hence it takes variable number of arguments, Row() is used to create the row object. Once the row object created, we can retrieve the data from Row using index similar to tuple.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8adb80a-34e7-4020-8dc2-5d0df4685197","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import Row"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e4e699f-8d2c-47f9-8473-a9f514606ed6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["row = Row(\"James\", 40)\nprint(row[0]+','+str(row[1]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bff5bdd3-601c-4571-919e-6862dcc2c4c7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"James,40\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["James,40\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**This outputs James,40. Alternatively you can also write with named arguments. Benefits with the named argument is you can access with field name row.name. Below example print “Alice”.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ea45619e-33ed-4d92-88fb-ab71a6e55c7f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["row = Row(name='Alice', age=11)\nprint(row.name)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"67b4a1ec-60ac-475f-b70e-41ddfbc39afe","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Alice\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Alice\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##2. Create Custom Class from Row\n\n---\n\n**We can also create a Row like class, for example “Person” and use it similar to Row object. This would be helpful when you wanted to create real time object and refer it’s properties. On below example, we have created a Person class and used similar to Row.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"84f26ef8-e70f-4070-9d0f-3161e8e72081","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["Person = Row(\"name\", \"age\")\n\np1 = Person(\"James\", 40)\np2 = Person(\"Alice\", 35)\n\nprint(p1.name+','+p2.name)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9fbc20df-cfb6-4fb7-ac24-b0b2a46c8e86","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"James,Alice\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["James,Alice\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#3. Using Row class on PySpark RDD\n\n---\n\n**We can use Row class on PySpark RDD. When you use Row to create an RDD, after collecting the data you will get the result back in Row.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b161c03b-4a7a-4f76-99f0-fa4d6234cb22","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import Row\n\ndata = [\n    Row(name='James,,Smith', lang=['Java','Scala','C++'], state='CA'),\n    Row(name=\"Michael, Rose,\", lang=['Spark', 'Java', 'C++'], state='NJ'),\n    Row(name=\"Robert,,Williams\", lang=['CSharp',\"VB\"], state='NV')\n       ]\nrdd = sc.parallelize(data)\nprint(rdd.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e2a98f3-1ca5-4929-a757-98d04dc2f87d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael, Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael, Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["***Now, let’s collect the data and access the data using its properties.***"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19c34d7f-41eb-46c5-a8c8-d11fc4985142","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["collData = rdd.collect()\n\nfor row in collData:\n    print(row.name + ',' + str(row.lang))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68935bf2-a511-41fe-88e3-b6e27a053882","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"James,,Smith,['Java', 'Scala', 'C++']\nMichael, Rose,,['Spark', 'Java', 'C++']\nRobert,,Williams,['CSharp', 'VB']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["James,,Smith,['Java', 'Scala', 'C++']\nMichael, Rose,,['Spark', 'Java', 'C++']\nRobert,,Williams,['CSharp', 'VB']\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["***Alternatively, you can also do by creating a Row like class “Person”***"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10d40f51-d4f9-4c8c-bbba-28869ded3d34","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["Person = Row('name', 'lang', 'state')\n\ndata_v2 = [\n    Person('James,,Smith',['java','Scala','C++'],'CA'),\n    Person('Michael,Rose,',['Spark','Java','C++'],\"NJ\"),\n    Person('Robert,,Williams',['CSharp','VB'],'NV')\n]\n\nrdd_v2 = sc.parallelize(data_v2)\nrdd_v2.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"be08d5e3-ff12-4ec8-a341-f2bcada8917a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[6]: [Row(name='James,,Smith', lang=['java', 'Scala', 'C++'], state='CA'),\n Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'),\n Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[6]: [Row(name='James,,Smith', lang=['java', 'Scala', 'C++'], state='CA'),\n Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'),\n Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]"]}}],"execution_count":0},{"cell_type":"code","source":["collData_v2 = rdd_v2.collect()\nfor row in collData_v2:\n    print(row.name, row.lang)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"383a7fcd-b65d-49b2-bf72-79983a85d0c6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"James,,Smith ['java', 'Scala', 'C++']\nMichael,Rose, ['Spark', 'Java', 'C++']\nRobert,,Williams ['CSharp', 'VB']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["James,,Smith ['java', 'Scala', 'C++']\nMichael,Rose, ['Spark', 'Java', 'C++']\nRobert,,Williams ['CSharp', 'VB']\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#4. Using Row class on PySpark DataFrame\n\n---\n\n**Similarly, Row class also can be used with PySpark DataFrame, By default data in DataFrame represent as Row. To demonstrate, I will use the same data that was created for RDD.**\n\n**Note that Row on DataFrame is not allowed to omit a named argument to represent that the value is None or missing. This should be explicitly set to None in this case.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1de0d629-4677-4ae9-b899-908cf0f358d4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.createDataFrame(data=data)\ndf.printSchema()\ndf.show()\n\n#This yields below output. Note that DataFrame able to take the column names from Row object."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"07ef57cf-773b-4884-8ae2-0e3abd3f775c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- lang: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n\n+----------------+------------------+-----+\n|            name|              lang|state|\n+----------------+------------------+-----+\n|    James,,Smith|[java, Scala, C++]|   CA|\n|   Michael,Rose,|[Spark, Java, C++]|   NJ|\n|Robert,,Williams|      [CSharp, VB]|   NV|\n+----------------+------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- lang: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n\n+----------------+------------------+-----+\n|            name|              lang|state|\n+----------------+------------------+-----+\n|    James,,Smith|[java, Scala, C++]|   CA|\n|   Michael,Rose,|[Spark, Java, C++]|   NJ|\n|Robert,,Williams|      [CSharp, VB]|   NV|\n+----------------+------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["***You can also change the column names by using toDF() function***"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e4b0511c-56fb-4b74-9851-b4fe7b8fc564","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["columns = ['name', 'languageAtSchool', 'currentState']\n\ndf= spark.createDataFrame(data=data).toDF(*columns)\ndf.printSchema()\n\n#This yields below output, note the column name “languagesAtSchool” from the previous example."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"21d842a7-cd5a-4d4e-b1d8-bf150443b1d2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- languageAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- languageAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#5. Create Nested Struct Using Row Class\n\n---\n\n**The below example provides a way to create a struct type using the Row class. Alternatively, you can also create struct type using By Providing Schema using PySpark StructType & StructFields**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d87893b3-25b3-4d64-a43e-588a16d23431","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Create Dataframe with struct using Row class\nfrom pyspark.sql import Row\n\ndata = [\n    Row(name='James', prop=Row(hair='black', eye='blue')),\n    Row(name='Ann', prop=Row(hair='grey', eye='balck'))\n]\n\ndf = spark.createDataFrame(data=data)\ndf.printSchema()\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ad75d38-0514-4eda-b79f-ee56aadebdee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+-----+-------------+\n| name|         prop|\n+-----+-------------+\n|James|{black, blue}|\n|  Ann|{grey, balck}|\n+-----+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+-----+-------------+\n| name|         prop|\n+-----+-------------+\n|James|{black, blue}|\n|  Ann|{grey, balck}|\n+-----+-------------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark Row using on DataFrame and RDD","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4007787913313843}},"nbformat":4,"nbformat_minor":0}
