{"cells":[{"cell_type":"markdown","source":["#PySpark Random Sample with Example\n\n\n---\n\n\n**PySpark provides a pyspark.sql.DataFrame.sample(), pyspark.sql.DataFrame.sampleBy(), RDD.sample(), and RDD.takeSample() methods to get the random sampling subset from the large dataset, In this article I will explain with Python examples.**\n\n**If you are working as a Data Scientist or Data analyst you are often required to analyze a large dataset/file with billions or trillions of records, processing these large datasets takes some time hence during the analysis phase it is recommended to use a random subset sample from the large files.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e2246da9-6ce9-4c8f-80a0-6067ef71882f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##1. PySpark SQL sample() Usage & Examples\n\n---\n\n\n**PySpark sampling (pyspark.sql.DataFrame.sample()) is a mechanism to get random sample records from the dataset, this is helpful when you have a larger dataset and wanted to analyze/test a subset of the data for example 10% of the original file.**\n\n\n\n---\n\n\n###Below is the syntax of the sample() function.\n\n\n##sample(withReplacement, fraction, seed=None)\n\n\n---\n\n- withReplacement – Sample with replacement or not (default False).\n\n- fraction – Fraction of rows to generate, range [0.0, 1.0]. Note that it doesn’t guarantee to provide the exact number of the fraction of records.\n\n- seed – Seed for sampling (default a random seed). Used to reproduce the same random sampling."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"72f746e9-bf83-4813-8807-87b975c9bde9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###1.1 Using fraction to get a random sample in PySpark\n\n\n**By using fraction between 0 to 1, it returns the approximate number of the fraction of the dataset. For example, 0.1 returns 10% of the rows. However, this does not guarantee it returns the exact 10% of the records.**\n\n---\n\n**Note: If you run these examples on your system, you may see different results.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8c4c2bfa-e5eb-4ff0-9151-aa0038a1c625","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.range(100)\nprint(df.sample(0.06).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cf41a7a9-c9dd-44dd-b9eb-250254d5475c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(id=3), Row(id=11), Row(id=31), Row(id=35), Row(id=53), Row(id=83), Row(id=95)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(id=3), Row(id=11), Row(id=31), Row(id=35), Row(id=53), Row(id=83), Row(id=95)]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**My DataFrame has 100 records and I wanted to get 6% sample records which are 6 but the sample() function returned 7 records. This proves the sample function doesn’t return the exact fraction specified.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59fb8bc5-b7cb-4065-972b-8bba77f687e6","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###1.2 Using seed to reproduce the same Samples in PySpark\n\n\n---\n\n\n**Every time you run a sample() function it returns a different set of sampling records, however sometimes during the development and testing phase you may need to regenerate the same sample every time as you need to compare the results from your previous run. To get consistent same random sampling uses the same slice value for every run. Change slice value to get different results.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"53359ad7-418b-4aba-a948-605eae1b44bb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(df.sample(0.1, 123).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b430a67e-de79-4baf-9284-c0c9911dca0b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(id=35), Row(id=38), Row(id=41), Row(id=45), Row(id=71), Row(id=84), Row(id=87), Row(id=99)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(id=35), Row(id=38), Row(id=41), Row(id=45), Row(id=71), Row(id=84), Row(id=87), Row(id=99)]\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(df.sample(0.1, 123).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8f55214d-b99d-40e1-a035-1feef810464f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(id=35), Row(id=38), Row(id=41), Row(id=45), Row(id=71), Row(id=84), Row(id=87), Row(id=99)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(id=35), Row(id=38), Row(id=41), Row(id=45), Row(id=71), Row(id=84), Row(id=87), Row(id=99)]\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(df.sample(0.1, 456).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b93d45d0-9bf2-45b2-8d97-4bbeee1b0366","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(id=22), Row(id=33), Row(id=35), Row(id=41), Row(id=53), Row(id=80), Row(id=83), Row(id=87), Row(id=92)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(id=22), Row(id=33), Row(id=35), Row(id=41), Row(id=53), Row(id=80), Row(id=83), Row(id=87), Row(id=92)]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Here, first 2 examples I have used seed value 123 hence the sampling results are the same and for the last example, I have used 456 as a seed value generate different sampling records.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b1ebfcd2-1dc0-4100-9bb5-8ae6070fa6d5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###1.3 Sample withReplacement (May contain duplicates)\n\n\n**some times you may need to get a random sample with repeated values. By using the value true, results in repeated values.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b600ccd9-baef-4e96-9265-dfeceae5e193","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(df.sample(True, 0.3, 123).collect())  # With Duplicates"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5911915a-6ad4-42e4-af2e-81cdab919696","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(id=0), Row(id=5), Row(id=9), Row(id=11), Row(id=13), Row(id=16), Row(id=17), Row(id=26), Row(id=26), Row(id=37), Row(id=41), Row(id=45), Row(id=49), Row(id=50), Row(id=50), Row(id=57), Row(id=58), Row(id=58), Row(id=65), Row(id=66), Row(id=71), Row(id=74), Row(id=77), Row(id=80), Row(id=81), Row(id=82), Row(id=84), Row(id=88), Row(id=90), Row(id=91), Row(id=91), Row(id=92), Row(id=94), Row(id=96)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(id=0), Row(id=5), Row(id=9), Row(id=11), Row(id=13), Row(id=16), Row(id=17), Row(id=26), Row(id=26), Row(id=37), Row(id=41), Row(id=45), Row(id=49), Row(id=50), Row(id=50), Row(id=57), Row(id=58), Row(id=58), Row(id=65), Row(id=66), Row(id=71), Row(id=74), Row(id=77), Row(id=80), Row(id=81), Row(id=82), Row(id=84), Row(id=88), Row(id=90), Row(id=91), Row(id=91), Row(id=92), Row(id=94), Row(id=96)]\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(df.sample(0.3, 123).collect())  # No Duplicates"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ebe08a48-7cca-45e7-86a3-8e3e70ce2506","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(id=0), Row(id=4), Row(id=12), Row(id=15), Row(id=19), Row(id=21), Row(id=23), Row(id=24), Row(id=25), Row(id=28), Row(id=29), Row(id=34), Row(id=35), Row(id=36), Row(id=38), Row(id=41), Row(id=45), Row(id=47), Row(id=50), Row(id=52), Row(id=59), Row(id=63), Row(id=65), Row(id=71), Row(id=82), Row(id=84), Row(id=87), Row(id=94), Row(id=99)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(id=0), Row(id=4), Row(id=12), Row(id=15), Row(id=19), Row(id=21), Row(id=23), Row(id=24), Row(id=25), Row(id=28), Row(id=29), Row(id=34), Row(id=35), Row(id=36), Row(id=38), Row(id=41), Row(id=45), Row(id=47), Row(id=50), Row(id=52), Row(id=59), Row(id=63), Row(id=65), Row(id=71), Row(id=82), Row(id=84), Row(id=87), Row(id=94), Row(id=99)]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**On first example, values 26, 50, 58 and 91 are repeated values.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"beccd6bd-f27b-4d8a-8382-551593293e8d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###1.4 Stratified sampling in PySpark\n\n\n**You can get Stratified sampling in PySpark without replacement by using sampleBy() method. It returns a sampling fraction for each stratum. If a stratum is not specified, it takes zero as the default.**\n\n\n\n---\n\n###sampleBy() Syntax\n\n\n##sampleBy(col, fractions, seed=None)\n\n\n\n- col – column name from DataFrame\n\n- fractions – It’s Dictionary type takes key and value.\n\n\n---\n\n\n###sampleBy() Example"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d6ac54a3-3218-44c2-a68f-bd98d748b343","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df2 = df.select((df.id % 3).alias(\"key\"))\n\nprint(df2.sampleBy(\"key\", {0: 0.1, 1:0.2}, 0).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"97297e47-e076-4668-bd13-ef6118143735","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(key=0), Row(key=0), Row(key=1), Row(key=1), Row(key=0), Row(key=1), Row(key=0), Row(key=1), Row(key=0), Row(key=0), Row(key=1), Row(key=1), Row(key=0)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(key=0), Row(key=0), Row(key=1), Row(key=1), Row(key=0), Row(key=1), Row(key=0), Row(key=1), Row(key=0), Row(key=0), Row(key=1), Row(key=1), Row(key=0)]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##2. PySpark RDD Sample\n\n\n**PySpark RDD also provides sample() function to get a random sampling, it also has another signature takeSample() that returns an Array[T].**\n\n\n---\n\n\n###RDD sample() Syntax & Example\n\n**PySpark RDD sample() function returns the random sampling similar to DataFrame and takes a similar types of parameters but in a different order. Since I’ve already covered the explanation of these parameters on DataFrame, I will not be repeating the explanation on RDD, If not already read I recommend reading the DataFrame section above.**\n\n**sample() of RDD returns a new RDD by selecting random sampling.** \n\n---\n\n###Below is a syntax.\n\n\n##sample(self, withReplacement, fraction, seed=None)\n\n---\n\n\n**Below is an example of RDD sample() function**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6568fa4a-3725-4d16-9559-3c33f03d00c5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["rdd = sc.range(0,100)\n\nprint(rdd.sample(False, 0.1, 0).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5a4ae97c-a55d-40a6-9e04-b10561517028","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[23, 48, 53, 60, 72, 87, 91, 96, 98]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[23, 48, 53, 60, 72, 87, 91, 96, 98]\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(rdd.sample(True, 0.3, 0).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c701bfa0-7846-4e28-b3d8-256593a55ce1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[2, 4, 4, 5, 7, 13, 15, 17, 23, 24, 25, 26, 29, 30, 30, 31, 31, 32, 37, 38, 42, 43, 45, 48, 52, 55, 57, 62, 68, 69, 73, 74, 74, 76, 82, 83, 84, 86, 93]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[2, 4, 4, 5, 7, 13, 15, 17, 23, 24, 25, 26, 29, 30, 30, 31, 31, 32, 37, 38, 42, 43, 45, 48, 52, 55, 57, 62, 68, 69, 73, 74, 74, 76, 82, 83, 84, 86, 93]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###RDD takeSample() Syntax & Example\n\n**RDD takeSample() is an action hence you need to careful when you use this function as it returns the selected sample records to driver memory. Returning too much data results in an out-of-memory error similar to collect().**\n\n\n---\n\n\n###Syntax of RDD takeSample() .\n\n\n##takeSample(self, withReplacement, num, seed=None) \n\n\n----\n\n\n**Example of RDD takeSample()**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"188dc6f7-3571-470e-b742-6322f9ea875b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(rdd.takeSample(False, 10, 0))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"828c404c-951a-4ce4-9678-6b5e9b34424d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[18, 60, 51, 68, 22, 1, 35, 84, 75, 72]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[18, 60, 51, 68, 22, 1, 35, 84, 75, 72]\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(rdd.takeSample(True, 30, 123))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"95f852ae-090b-4195-a10c-9e11b43dbae7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[72, 91, 55, 86, 37, 49, 34, 46, 63, 21, 81, 17, 20, 84, 29, 46, 84, 14, 59, 7, 80, 25, 60, 59, 54, 22, 34, 83, 82, 25]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[72, 91, 55, 86, 37, 49, 34, 46, 63, 21, 81, 17, 20, 84, 29, 46, 84, 14, 59, 7, 80, 25, 60, 59, 54, 22, 34, 83, 82, 25]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Conclusion\n\n\n**In summary, PySpark sampling can be done on RDD and DataFrame. In order to do sampling, you need to know how much data you wanted to retrieve by specifying fractions.**\n\n**Use seed to regenerate the same sampling multiple times. and**\n\n**Use withReplacement if you are okay to repeat the random records.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b843bc7-c15f-4c98-bfc5-54a47d710433","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark Random Sample with Example","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4425648766185697}},"nbformat":4,"nbformat_minor":0}
