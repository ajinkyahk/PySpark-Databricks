{"cells":[{"cell_type":"markdown","source":["#PySpark UDF (User Defined Function)\n\n\n---\n\n\n![pyspark_udf image](/files/pyspark_udf.jpg)\n\n\n---\n\n\n**PySpark UDF (a.k.a User Defined Function) is the most useful feature of Spark SQL & DataFrame that is used to extend the PySpark build in capabilities. In this article, I will explain what is UDF? why do we need it and how to create and use it on DataFrame select(), withColumn() and SQL using PySpark (Spark with Python) examples.**\n\n**Note: UDF’s are the most expensive operations hence use them only you have no choice and when essential. In the later section of the article, I will explain why using UDF’s is an expensive operation in detail.**\n\n\n---\n\n\n##Table of contents\n\n- PySpark UDF Introduction\n  - What is UDF?\n  - Why do we need it?\n- Create PySpark UDF (User Defined Function)\n  - Create a DataFrame\n  - Create a Python function\n  - Convert python function to UDF\n- Using UDF with DataFrame\n  - Using UDF with DataFrame select()\n  - Using UDF with DataFrame withColumn()\n  - Registring UDF & Using it on SQL query\n- Create UDF using annotation\n- Special handling\n  - Null check\n  - Performance concern"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4d69676-1fec-41bc-bf40-4315d60e07bc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##1. PySpark UDF Introduction\n\n\n--- \n\n\n###1.1 What is UDF?\n\n**UDF’s a.k.a User Defined Functions, If you are coming from SQL background, UDF’s are nothing new to you as most of the traditional RDBMS databases support User Defined Functions, these functions need to register in the database library and use them on SQL as regular functions.**\n\n**PySpark UDF’s are similar to UDF on traditional databases. In PySpark, you create a function in a Python syntax and wrap it with PySpark SQL udf() or register it as udf and use it on DataFrame and SQL respectively.**\n\n\n---\n\n###1.2 Why do we need a UDF?\n\n**UDF’s are used to extend the functions of the framework and re-use these functions on multiple DataFrame’s. For example, you wanted to convert every first letter of a word in a name string to a capital case; PySpark build-in features don’t have this function hence you can create it a UDF and reuse this as needed on many Data Frames. UDF’s are once created they can be re-used on several DataFrame’s and SQL expressions.**\n\n**Before you create any UDF, do your research to check if the similar function you wanted is already available in Spark SQL Functions. PySpark SQL provides several predefined common functions and many more new functions are added with every release. hence, It is best to check before you reinventing the wheel.**\n\n***When you creating UDF’s you need to design them very carefully otherwise you will come across optimization & performance issues.***"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f66ca94b-b114-44b5-b7f0-b50cfcff4ff0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##2. Create PySpark UDF\n\n###2.1 Create a DataFrame\n\n**Before we jump in creating a UDF, first let’s create a PySpark DataFrame.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e2f94ef6-ffe5-4c22-95f3-ec4478b408ee","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["columns = [\"Seqno\", \"Name\"]\n\ndata = [\n    (\"1\", \"john james\"),\\\n    (\"2\", \"tracey smith\"),\\\n    (\"3\", \"amy sanders\")\n]\n\ndf = spark.createDataFrame(data=data, schema=columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e5764fb-f666-4c19-8e67-4c526be51ca8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Seqno: string (nullable = true)\n |-- Name: string (nullable = true)\n\n+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john james  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Seqno: string (nullable = true)\n |-- Name: string (nullable = true)\n\n+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john james  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###2.2 Create a Python Function\n\n\n---\n\n**The first step in creating a UDF is creating a Python function. Below snippet creates a function convertCase() which takes a string parameter and converts the first letter of every word to capital letter. UDF’s take parameters of your choice and returns a value.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"64c0ecc6-70bd-4389-af76-033c94e883e9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def convertCase(name_str):\n    resStr = \"\"\n    arr = name_str.split(\" \")\n    for x in arr:\n        resStr = resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n    return resStr\n\nconvertCase(\"ajinkya kulkarni\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4096033e-9afa-4fcd-b879-a14cc3268ff4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[15]: 'Ajinkya Kulkarni '","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[15]: 'Ajinkya Kulkarni '"]}}],"execution_count":0},{"cell_type":"markdown","source":["**The first step in creating a UDF is creating a Python function. Below snippet creates a function convertCase() which takes a string parameter and converts the first letter of every word to capital letter. UDF’s take parameters of your choice and returns a value.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dc6b2194-1b84-430e-aa2a-fbb502e8d4ab","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##2.3 Convert a Python function to PySpark UDF\n\n\n**Now convert this function convertCase() to UDF by passing the function to PySpark SQL udf(), this function is available at org.apache.spark.sql.functions.udf package. Make sure you import this package before using it.**\n\n**PySpark SQL udf() function returns org.apache.spark.sql.expressions.UserDefinedFunction class object.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"66f6cece-8cc5-46b4-91df-111227ed5ee5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import StringType\n\n#Converting function to UDF\nconvertUDF = udf(lambda z: convertCase(z), StringType())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d153317-cb9f-4c6e-aa12-7d27a65c8131","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Note: The default type of the udf() is StringType hence, you can also write the above statement without return type.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90c33fa8-89b5-44b6-9f03-af5569d0f736","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Converting function to UDF\n#StringType() is by default hence not required\n\nconvertUDF = udf(lambda z : convertCase(z))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce9f0f29-5125-437c-9c96-f0886a30c310","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##3. Using UDF with DataFrame\n\n---\n\n###3.1 Using UDF with PySpark DataFrame select()\n\n\n---\n\n\n**Now you can use convertUDF() on a DataFrame column as a regular build-in function.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d2427d9-ccc6-4af9-9e1a-1a8e96570895","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(col(\"Seqno\"),\\\nconvertUDF(col(\"name\")).alias(\"Name\"))\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ae08b46-0acc-4052-a07f-d1f589557ce6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+-------------+\n|Seqno|Name         |\n+-----+-------------+\n|1    |John James   |\n|2    |Tracey Smith |\n|3    |Amy Sanders  |\n+-----+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+-------------+\n|Seqno|Name         |\n+-----+-------------+\n|1    |John James   |\n|2    |Tracey Smith |\n|3    |Amy Sanders  |\n+-----+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###3.2 Using UDF with PySpark DataFrame withColumn()\n\n\n---\n\n\n**You could also use udf on DataFrame withColumn() function, to explain this I will create another upperCase() function which converts the input string to upper case.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2fde3bc-e754-4dd4-baeb-a1e3bb2a6752","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def upperCase(name_str):\n    return name_str.upper()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6000db36-80eb-46fa-a064-afa6dcdc2048","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Let’s convert upperCase() python function to UDF and then use it with DataFrame withColumn(). Below example converts the values of “Name” column to upper case and creates a new column “Curated Name”**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46e243ad-e76c-4fae-8ecf-442463a73886","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["upperCaseUDF = udf(lambda z:upperCase(z), StringType())\n\ndf.withColumn(\"Curated Name\", upperCaseUDF(col(\"Name\")))\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cdb882cf-b9ff-4d62-8081-2d1a0f69431d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------------+------------+\n|Seqno|Name        |Curated Name|\n+-----+------------+------------+\n|1    |john james  |JOHN JAMES  |\n|2    |tracey smith|TRACEY SMITH|\n|3    |amy sanders |AMY SANDERS |\n+-----+------------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------------+------------+\n|Seqno|Name        |Curated Name|\n+-----+------------+------------+\n|1    |john james  |JOHN JAMES  |\n|2    |tracey smith|TRACEY SMITH|\n|3    |amy sanders |AMY SANDERS |\n+-----+------------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###3.3 Registering PySpark UDF & use it on SQL\n\n\n---\n\n\n**In order to use convertCase() function on PySpark SQL, you need to register the function with PySpark by using spark.udf.register().**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"55e52d9e-c050-484a-8cf0-76c7ae94004a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["\"\"\" Using UDF on SQL \"\"\"\n\nspark.udf.register(\"convertUDF\", convertCase, StringType())\n\ndf.createOrReplaceTempView(\"NAME_TABLE\")\nspark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\")\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"601e422a-58ca-46ea-8311-7e2987f396b1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+-------------+\n|Seqno|Name         |\n+-----+-------------+\n|1    |John James   |\n|2    |Tracey Smith |\n|3    |Amy Sanders  |\n+-----+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+-------------+\n|Seqno|Name         |\n+-----+-------------+\n|1    |John James   |\n|2    |Tracey Smith |\n|3    |Amy Sanders  |\n+-----+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##4. Creating UDF using annotation\n\n\n---\n\n\n**In the previous sections, you have learned creating a UDF is a 2 step process, first, you need to create a Python function, second convert function to UDF using SQL udf() function, however, you can avoid these two steps and create it with just a single step by using annotations.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7cc93e76-0429-435e-b59e-f8b4149dfbab","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["@udf(returnType=StringType())\ndef upperCase(name_str):\n    return name_str.upper()\n\ndf.withColumn(\"Curated Name\", upperCase(col(\"Name\")))\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f4f5bf31-0856-4c62-831f-765ca013d700","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------------+------------+\n|Seqno|Name        |Curated Name|\n+-----+------------+------------+\n|1    |john james  |JOHN JAMES  |\n|2    |tracey smith|TRACEY SMITH|\n|3    |amy sanders |AMY SANDERS |\n+-----+------------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------------+------------+\n|Seqno|Name        |Curated Name|\n+-----+------------+------------+\n|1    |john james  |JOHN JAMES  |\n|2    |tracey smith|TRACEY SMITH|\n|3    |amy sanders |AMY SANDERS |\n+-----+------------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##5. Special Handling\n\n---\n\n\n###5.1 Execution order\n\n**One thing to aware is in PySpark/Spark does not guarantee the order of evaluation of subexpressions meaning expressions are not guarantee to evaluated left-to-right or in any other fixed order. PySpark reorders the execution for query optimization and planning hence, AND, OR, WHERE and HAVING expression will have side effects.**\n\n**So when you are designing and using UDF, you have to be very careful especially with null handling as these results runtime exceptions.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3442c86f-78ca-4e74-b503-8dc0013606ef","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["\"\"\"\nNo guarantee Name is not null will execute first\nIf convertUDF(Name) like '%John%' execute first then \nyou will get runtime error\n\"\"\"\n\nspark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE \" +\\\n         \"where Name is not null and convertUDF(Name) like '%John%'\")\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"44783d79-525a-4327-babd-49f0bcaa1350","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+-----------+\n|Seqno|Name       |\n+-----+-----------+\n|1    |John James |\n+-----+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+-----------+\n|Seqno|Name       |\n+-----+-----------+\n|1    |John James |\n+-----+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###5.2 Handling null check\n\n---\n\n\n**UDF’s are error-prone when not designed carefully. for example, when you have a column that contains the value null on some records**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"93211456-dcfa-4184-a7c2-529b44039366","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["\"\"\" null check \"\"\"\n\ncolumns = [\"Seqno\", \"Name\"]\n\ndata = [\n    (\"1\", \"john jones\"),\n    (\"2\", \"tracey smith\"),\n    (\"3\", \"amy sanders\"),\n    ('4',None)\n]\n\ndf2 = spark.createDataFrame(data=data, schema=columns)\ndf2.show(truncate=False)\ndf2.createOrReplaceTempView(\"NAME_TABLE2\")\n\nspark.sql(\"select convertUDF(Name) from NAME_TABLE2\")\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8a8d164e-32be-45bb-9570-5ebba788b342","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n|4    |null        |\n+-----+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n|4    |null        |\n+-----+------------+\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPythonException\u001B[0m                           Traceback (most recent call last)\nFile \u001B[0;32m<command-2656410894484160>:16\u001B[0m\n\u001B[1;32m     13\u001B[0m df2\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     14\u001B[0m df2\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNAME_TABLE2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 16\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mselect convertUDF(Name) from NAME_TABLE2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m     17\u001B[0m \u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:620\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    615\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    616\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    617\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParameter \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtruncate=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m should be either bool or int.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(truncate)\n\u001B[1;32m    618\u001B[0m     )\n\u001B[0;32m--> 620\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mint_truncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:202\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    198\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 202\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mPythonException\u001B[0m: An exception was thrown from a UDF: 'AttributeError: 'NoneType' object has no attribute 'split'', from <command-2656410894484141>, line 3. Full traceback below:\nTraceback (most recent call last):\n  File \"<command-2656410894484141>\", line 3, in convertCase\nAttributeError: 'NoneType' object has no attribute 'split'\n","errorSummary":"<span class='ansi-red-fg'>PythonException</span>: An exception was thrown from a UDF: 'AttributeError: 'NoneType' object has no attribute 'split'', from <command-2656410894484141>, line 3. Full traceback below:\nTraceback (most recent call last):\n  File \"<command-2656410894484141>\", line 3, in convertCase\nAttributeError: 'NoneType' object has no attribute 'split'\n","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPythonException\u001B[0m                           Traceback (most recent call last)\nFile \u001B[0;32m<command-2656410894484160>:16\u001B[0m\n\u001B[1;32m     13\u001B[0m df2\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     14\u001B[0m df2\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNAME_TABLE2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 16\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mselect convertUDF(Name) from NAME_TABLE2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m     17\u001B[0m \u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:620\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    615\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    616\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    617\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParameter \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtruncate=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m should be either bool or int.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(truncate)\n\u001B[1;32m    618\u001B[0m     )\n\u001B[0;32m--> 620\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mint_truncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:202\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    198\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 202\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mPythonException\u001B[0m: An exception was thrown from a UDF: 'AttributeError: 'NoneType' object has no attribute 'split'', from <command-2656410894484141>, line 3. Full traceback below:\nTraceback (most recent call last):\n  File \"<command-2656410894484141>\", line 3, in convertCase\nAttributeError: 'NoneType' object has no attribute 'split'\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Note that from the above snippet, record with “Seqno 4” has value “None” for “name” column. Since we are not handling null with UDF function, using this on DataFrame returns below error. Note that in Python None is considered null.**\n\n\n---\n\n\n####Below points to remember\n\n- Its always best practice to check for null inside a UDF function rather than checking for null outside.\n- In any case, if you can’t do a null check in UDF at lease use IF or CASE WHEN to check for null and call UDF conditionally."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"608fe109-a2b6-4a6e-b059-8773105ed62f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.udf.register(\"_nullsafeUDF\", lambda name_str: convertCase(name_str) if not name_str is None else \"\", StringType())\n\nspark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\")\\\n.show(truncate=False)\n\nspark.sql(\" select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2\" + \\\n         \" where Name is not null and _nullsafeUDF(Name) like '%John%' \")\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3ab54c5c-9a2d-414e-b314-a79f7b84749b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------+\n|_nullsafeUDF(Name)|\n+------------------+\n|John Jones        |\n|Tracey Smith      |\n|Amy Sanders       |\n|                  |\n+------------------+\n\n+-----+-----------+\n|Seqno|Name       |\n+-----+-----------+\n|1    |John Jones |\n+-----+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------+\n|_nullsafeUDF(Name)|\n+------------------+\n|John Jones        |\n|Tracey Smith      |\n|Amy Sanders       |\n|                  |\n+------------------+\n\n+-----+-----------+\n|Seqno|Name       |\n+-----+-----------+\n|1    |John Jones |\n+-----+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**This executes successfully without errors as we are checking for null/none while registering UDF.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"696b9034-edf0-447b-b852-723b3bc383e8","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###5.3 Performance concern using UDF\n\n\n---\n\n\n**UDFs are a black box to PySpark hence it can’t apply optimization and you will lose all the optimization PySpark does on Dataframe/Dataset. When possible you should use Spark SQL built-in functions as these functions provide optimization. Consider creating UDF only when the existing built-in SQL function doesn’t have it.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1667d6e8-cd5a-421d-b442-6467462d554b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def myfun(nm_str):\n    print(not nm_str is None)\n    if not nm_str is None :\n        return nm_str.upper()\n    else:\n        return ''"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b32bdb3-8109-4d20-a4f7-bfc0bf9c2f5a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["myfun(None)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"468f5c43-9e6f-448d-b5a5-2c63505358d1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"False\nOut[53]: ''","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["False\nOut[53]: ''"]}}],"execution_count":0},{"cell_type":"code","source":["myfun('ak')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9946c6e-6867-4715-b89a-17e227775d11","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"True\nOut[55]: 'AK'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["True\nOut[55]: 'AK'"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark UDF (User Defined Function)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2656410894484135}},"nbformat":4,"nbformat_minor":0}
