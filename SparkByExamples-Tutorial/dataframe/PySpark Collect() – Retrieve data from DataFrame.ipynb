{"cells":[{"cell_type":"markdown","source":["#PySpark Collect() – Retrieve data from DataFrame\n\n---\n\n**PySpark RDD/DataFrame collect() is an action operation that is used to retrieve all the elements of the dataset (from all nodes) to the driver node. We should use the collect() on smaller dataset usually after filter(), group() e.t.c. Retrieving larger datasets results in OutOfMemory error.**\n\n**In this PySpark article, I will explain the usage of collect() with DataFrame example, when to avoid it, and the difference between collect() and select().**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3720b0db-1efc-4371-bf0b-ccf92994de1e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dept = [\n    (\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n]\n\ndeptColumns = [\"dept_name\", \"dept_id\"]\n\ndeptDF = spark.createDataFrame(data = dept, schema = deptColumns)\ndeptDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3fb09c46-baac-4742-bdf9-ba69de4845a7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**show() function on DataFrame prints the result of DataFrame in a table format. By default, it shows only 20 rows. The above snippet returns the data in a table.**\n\n---\n\n###Now, let’s use the collect() to retrieve the data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"21701ba3-3184-4aa2-b63a-5807e0de054e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dataCollect = deptDF.collect()\nprint(dataCollect)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8464644-f3b4-4040-8747-2760ec3139b4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Note that collect() is an action hence it does not return a DataFrame instead, it returns data in an Array to the driver. Once the data is in an array, you can use python for loop to process it further.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f36a4d40-35ba-47f1-9b4e-bc82f733b7bc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["for row in dataCollect:\n    print(row['dept_name']+ \",\" + str(row['dept_id']))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d680328a-cb27-436f-b361-d273dd11f592","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Finance,10\nMarketing,20\nSales,30\nIT,40\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Finance,10\nMarketing,20\nSales,30\nIT,40\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["####If you wanted to get first row and first column from a DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5335aab6-00d3-4721-ad7c-024eb718e473","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Returns value of First Row, First Column which is \"Finance\"\ndeptDF.collect()[0][0]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ced11533-0c59-4986-a42f-18e15699a5c3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[9]: 'Finance'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[9]: 'Finance'"]}}],"execution_count":0},{"cell_type":"markdown","source":["####Let’s understand what’s happening on above statement.\n\n\n- deptDF.collect() returns Array of Row type.\n- deptDF.collect()[0] returns the first element in an array (1st row).\n- deptDF.collect[0][0] returns the value of the first row & first column.\n\n\n**In case you want to just return certain elements of a DataFrame, you should call PySpark select() transformation first.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c9c95005-73dc-4432-95b1-aba373dbdab6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dataCollect = deptDF.select(\"dept_name\").collect()\ndataCollect"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b846a25c-b6e2-4d02-8846-cbad3bea2a54","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[11]: [Row(dept_name='Finance'),\n Row(dept_name='Marketing'),\n Row(dept_name='Sales'),\n Row(dept_name='IT')]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: [Row(dept_name='Finance'),\n Row(dept_name='Marketing'),\n Row(dept_name='Sales'),\n Row(dept_name='IT')]"]}}],"execution_count":0},{"cell_type":"markdown","source":["---\n##When to avoid Collect()\n\n**Usually, collect() is used to retrieve the action output when you have very small result set and calling collect() on an RDD/DataFrame with a bigger result set causes out of memory as it returns the entire dataset (from all workers) to the driver hence we should avoid calling collect() on a larger dataset.**\n\n---\n\n##collect () vs select ()\n\n**select() is a transformation that returns a new DataFrame and holds the columns that are selected whereas collect() is an action that returns the entire data set in an Array to the driver.**\n\n\n---"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8532c107-1b2b-428c-8a2d-1ec374654329","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark Collect() – Retrieve data from DataFrame","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1005060144152413}},"nbformat":4,"nbformat_minor":0}
