{"cells":[{"cell_type":"markdown","source":["#PySpark lit() – Add Literal or Constant to DataFrame\n\n---\n\n**PySpark SQL functions lit() and typedLit() are used to add a new column to DataFrame by assigning a literal or constant value. Both these functions return Column type as return type.**\n\n**Both of these are available in PySpark by importing pyspark.sql.functions**\n\n\n**First, let’s create a DataFrame.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a409b2df-9ad8-472d-a2e2-50a0af720eb6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import lit, col, when"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"16b2cef6-dd72-40f9-ac5b-c4ee90361063","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\ncolumns= [\"EmpId\",\"Salary\"]\n\ndf = spark.createDataFrame(data=data, schema=columns)\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fdee94b3-5e44-4ca9-8fb8-2dd3ae5b15b0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- EmpId: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- EmpId: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##lit() Function to Add Constant Column\n\n\n**PySpark lit() function is used to add constant or literal value as a new column to the DataFrame.**\n\n\n\n---\n\n\n##Example 1: Simple usage of lit() function\n\n**Let’s see an example of how to create a new column with constant value using lit() Spark SQL function. On the below snippet, we are creating a new column by adding a literal ‘1’ to PySpark DataFrame.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"da2117d0-3e23-4402-938e-9722dbcb6c24","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df2 = df.select(col(\"EmpId\"), col(\"Salary\"), lit(\"1\").alias(\"lit_value1\"))\ndf2.printSchema()\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59f82706-5d40-405f-be22-d7e074dd58e0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- EmpId: string (nullable = true)\n |-- Salary: long (nullable = true)\n |-- lit_value1: string (nullable = false)\n\n+-----+------+----------+\n|EmpId|Salary|lit_value1|\n+-----+------+----------+\n|111  |50000 |1         |\n|222  |60000 |1         |\n|333  |40000 |1         |\n+-----+------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- EmpId: string (nullable = true)\n |-- Salary: long (nullable = true)\n |-- lit_value1: string (nullable = false)\n\n+-----+------+----------+\n|EmpId|Salary|lit_value1|\n+-----+------+----------+\n|111  |50000 |1         |\n|222  |60000 |1         |\n|333  |40000 |1         |\n+-----+------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Adding the same constant literal to all records in DataFrame may not be real-time useful so let’s see another example.**\n\n\n---\n\n\n##Example 2 : lit() function with withColumn\n\n\n**The following example shows how to use pyspark lit() function using withColumn to derive a new column based on some conditions.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"527f1e60-0ae9-4db9-aa95-271eb18f521c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df3 = df2.withColumn(\"lit_value2\", when((col(\"Salary\") >= 40000) & (col(\"Salary\") <= 50000),lit(\"100\")).otherwise(lit(\"200\")))\ndf3.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"47e9e4d0-bddc-4d56-ab76-4cfe9f8974e0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------+----------+----------+\n|EmpId|Salary|lit_value1|lit_value2|\n+-----+------+----------+----------+\n|111  |50000 |1         |100       |\n|222  |60000 |1         |200       |\n|333  |40000 |1         |100       |\n+-----+------+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------+----------+----------+\n|EmpId|Salary|lit_value1|lit_value2|\n+-----+------+----------+----------+\n|111  |50000 |1         |100       |\n|222  |60000 |1         |200       |\n|333  |40000 |1         |100       |\n+-----+------+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##typedLit() Function – Syntax\n\n\n**Difference between lit() and typedLit() is that, typedLit function can handle collection types e.g.: Array, Dictionary(map) e.t.c. Unfortunately, I could not find this function in PySpark, when I find it, I will add an example.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"65d1151a-a8cd-4bfe-80c2-0326d249d2ae","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark – lit()","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1848772988164676}},"nbformat":4,"nbformat_minor":0}
