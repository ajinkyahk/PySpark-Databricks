{"cells":[{"cell_type":"markdown","source":["#PySpark Replace Column Values in DataFrame\n\n---\n\n**You can replace column values of PySpark DataFrame by using SQL string functions regexp_replace(), translate(), and overlay() with Python examples.**\n\n**In this article, I will cover examples of how to replace part of a string with another string, replace all columns, change values conditionally, replace values from a python dictionary, replace column value from another DataFrame column e.t.c**\n\n\n**First, let’s create a PySpark DataFrame with some addresses and will use this DataFrame to explain how to replace column values.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"111ad0e0-19d2-4f5e-82bc-19c16dbce1c2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["address =  [(1,\"14851 Jeffrey Rd\",\"DE\"),\n    (2,\"43421 Margarita St\",\"NY\"),\n    (3,\"13111 Siemon Ave\",\"CA\")]\n\ndf = spark.createDataFrame(address, [\"id\", \"address\", \"state\"])\n\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43e3ebc6-234f-4998-9595-70d114135bdf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+------------------+-----+\n|id |address           |state|\n+---+------------------+-----+\n|1  |14851 Jeffrey Rd  |DE   |\n|2  |43421 Margarita St|NY   |\n|3  |13111 Siemon Ave  |CA   |\n+---+------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+------------------+-----+\n|id |address           |state|\n+---+------------------+-----+\n|1  |14851 Jeffrey Rd  |DE   |\n|2  |43421 Margarita St|NY   |\n|3  |13111 Siemon Ave  |CA   |\n+---+------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##1. PySpark Replace String Column Values\n\n\n**By using PySpark SQL function regexp_replace() you can replace a column value with a string for another string/substring. regexp_replace() uses Java regex for matching, if the regex does not match it returns an empty string, the below example replace the street name Rd value with Road string on address column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ccdd0523-9ad1-432e-9d21-93f0809a6fe0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Replace part of string with another string\nfrom pyspark.sql.functions import regexp_replace\n\ndf.withColumn('address', regexp_replace('address', 'Rd', 'Road'))\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc977603-9c66-438e-b345-852f0502a97a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+------------------+-----+\n|id |address           |state|\n+---+------------------+-----+\n|1  |14851 Jeffrey Road|DE   |\n|2  |43421 Margarita St|NY   |\n|3  |13111 Siemon Ave  |CA   |\n+---+------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+------------------+-----+\n|id |address           |state|\n+---+------------------+-----+\n|1  |14851 Jeffrey Road|DE   |\n|2  |43421 Margarita St|NY   |\n|3  |13111 Siemon Ave  |CA   |\n+---+------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##2. Replace Column Values Conditionally\n\n\n**In the above example, we just replaced Rd with Road, but not replaced St and Ave values, let’s see how to replace column values conditionally in PySpark Dataframe by using when().otherwise() SQL condition function.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e58fefb-2399-4179-8a43-425e6b5ae559","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Replace string column value conditionally\n\nfrom pyspark.sql.functions import when\n\ndf.withColumn('address', when(df.address.endswith('Rd'), regexp_replace(df.address, 'Rd', 'Road'))\\\n             .when(df.address.endswith('St'), regexp_replace(df.address, 'St', 'Street'))\\\n             .when(df.address.endswith('Ave'), regexp_replace(df.address, 'Ave', 'Avenue'))\\\n             .otherwise(df.address))\\\n             .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18ee4d7d-eb63-49fb-906c-d4584c8965f3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------------------+-----+\n|id |address               |state|\n+---+----------------------+-----+\n|1  |14851 Jeffrey Road    |DE   |\n|2  |43421 Margarita Street|NY   |\n|3  |13111 Siemon Avenue   |CA   |\n+---+----------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------------------+-----+\n|id |address               |state|\n+---+----------------------+-----+\n|1  |14851 Jeffrey Road    |DE   |\n|2  |43421 Margarita Street|NY   |\n|3  |13111 Siemon Avenue   |CA   |\n+---+----------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##3. Replace Column Value with Dictionary (map)\n\n\n**You can also replace column values from the python dictionary (map). In the below example, we replace the string value of the state column with the full abbreviated name from a dictionary key-value pair, in order to do so I use PySpark map() transformation to loop through each row of DataFrame.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35ae870e-4784-4498-bb9f-36cfd380d804","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Replace values from Dictionary\n\nstateDic = {'CA':'California','NY':'New York','DE':'Delaware'}\n\ndf2 = df.rdd.map(lambda x: ( x.id, x.address, stateDic[x.state]) ).toDF([\"id\", \"address\", \"state\"])\n\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0701b044-f441-4191-827b-0324cf2a0d50","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+------------------+----------+\n|id |address           |state     |\n+---+------------------+----------+\n|1  |14851 Jeffrey Rd  |Delaware  |\n|2  |43421 Margarita St|New York  |\n|3  |13111 Siemon Ave  |California|\n+---+------------------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+------------------+----------+\n|id |address           |state     |\n+---+------------------+----------+\n|1  |14851 Jeffrey Rd  |Delaware  |\n|2  |43421 Margarita St|New York  |\n|3  |13111 Siemon Ave  |California|\n+---+------------------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##4. Replace Column Value Character by Character\n\n\n**By using translate() string function you can replace character by character of DataFrame column value. In the below example, every character of 1 is replaced with A, 2 replaced with B, and 3 replaced with C on the address column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"797960de-b672-414d-91d8-1526dbbe1169","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Using translate to replace character by character\nfrom pyspark.sql.functions import translate\n\ndf.withColumn('address', translate('address', '123', 'ABC'))\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a1d54b86-823d-43fd-9a51-2b6182b1a10d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+------------------+-----+\n|id |address           |state|\n+---+------------------+-----+\n|1  |A485A Jeffrey Rd  |DE   |\n|2  |4C4BA Margarita St|NY   |\n|3  |ACAAA Siemon Ave  |CA   |\n+---+------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+------------------+-----+\n|id |address           |state|\n+---+------------------+-----+\n|1  |A485A Jeffrey Rd  |DE   |\n|2  |4C4BA Margarita St|NY   |\n|3  |ACAAA Siemon Ave  |CA   |\n+---+------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##5. Replace Column with Another Column Value\n\n\n**By using expr() and regexp_replace() you can replace column value with a value from another DataFrame column. In the below example, we match the value from col2 in col1 and replace with col3 to create new_column. Use expr() to provide SQL like expressions and is used to refer to another column to perform operations.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73a89c96-0b53-4195-be62-d3bbf8a8ff63","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Replace column with another column\n\nfrom pyspark.sql.functions import expr\n\ndf = spark.createDataFrame([(\"ABCDE_XYZ\", \"XYZ\",\"FGH\")], \n    (\"col1\", \"col2\",\"col3\"))\n\ndf.withColumn(\"new_column\", expr(\"regexp_replace(col1, col2, col3)\").alias(\"replaced_value\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cd9ce6f1-7cdd-4c63-a97e-ba999de594a6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+----+----+----------+\n|col1     |col2|col3|new_column|\n+---------+----+----+----------+\n|ABCDE_XYZ|XYZ |FGH |ABCDE_FGH |\n+---------+----+----+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+----+----+----------+\n|col1     |col2|col3|new_column|\n+---------+----+----+----------+\n|ABCDE_XYZ|XYZ |FGH |ABCDE_FGH |\n+---------+----+----+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##6. Using overlay() Function\n\n**Replace column value with a string value from another column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b861ee05-a537-4019-9e90-910bccfdda64","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#overlay\n\nfrom pyspark.sql.functions import overlay\ndf = spark.createDataFrame([(\"ABCDE_XYZ\", \"FGH\")], (\"col1\", \"col2\"))\ndf.select(overlay(\"col1\", \"col2\", 7).alias(\"overlayed\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fd55690c-4f58-4083-8a70-c6ae5e39ff44","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+\n|overlayed|\n+---------+\n|ABCDE_FGH|\n+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+\n|overlayed|\n+---------+\n|ABCDE_FGH|\n+---------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark – translate()","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2450113531380158}},"nbformat":4,"nbformat_minor":0}
