{"cells":[{"cell_type":"markdown","source":["#PySpark SQL expr() (Expression ) Function\n\n---\n\n**PySpark expr() is a SQL function to execute SQL-like expressions and to use an existing DataFrame column value as an expression argument to Pyspark built-in functions. Most of the commonly used SQL functions are either part of the PySpark Column class or built-in pyspark.sql.functions API, besides these PySpark also supports many other SQL functions, so in order to use these, you have to use expr() function.**\n\n\n---\n\n\n**Below are 2 use cases of PySpark expr() funcion.**\n\n- First, allowing to use of SQL-like functions that are not present in PySpark Column type & pyspark.sql.functions API. for example CASE WHEN, regr_count().\n\n- Second, it extends the PySpark SQL Functions by allowing to use DataFrame columns in functions for expression. for example, if you wanted to add a month value from a column to a Date column. I will explain this in the example below.\n\n\n---\n\n\n##1. PySpark expr() Syntax\n\n**Following is syntax of the expr() function.**\n\n\n###expr(str)\n\n**expr() function takes SQL expression as a string argument, executes the expression, and returns a PySpark Column type. Expressions provided with this function are not a compile-time safety like DataFrame operations.**\n\n\n--- \n\n\n##2. PySpark SQL expr() Function Examples\n\n**Below are some of the examples of using expr() SQL function.**\n\n\n---\n\n\n###2.1 Concatenate Columns using || (similar to SQL)\n\n\n**If you have SQL background, you pretty much familiar using || to concatenate values from two string columns, you can use expr() expression to do exactly same.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c9051ef-11ea-4969-8fd5-60222e25590c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import expr"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9eb7f26-8bae-4b0b-96d5-33b2ac1c4c81","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Concatenate columns using || (sql like)\n\ndata = [(\"James\", \"Bond\"), (\"Scott\", \"Varsa\")]\n\ndf = spark.createDataFrame(data, schema=[\"col1\", \"col2\"])\ndf.withColumn(\"Name\", expr(\" col1 ||','|| col2 \")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b6dc4e9-2e51-4b94-94e9-385f84495497","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+-----+-----------+\n|col1 |col2 |Name       |\n+-----+-----+-----------+\n|James|Bond |James,Bond |\n|Scott|Varsa|Scott,Varsa|\n+-----+-----+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+-----+-----------+\n|col1 |col2 |Name       |\n+-----+-----+-----------+\n|James|Bond |James,Bond |\n|Scott|Varsa|Scott,Varsa|\n+-----+-----+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###2.2 Using SQL CASE WHEN with expr()\n\n\n**PySpark doesn’t have SQL Like CASE WHEN so in order to use this on PySpark DataFrame withColumn() or select(), you should use expr() function with expression as shown below.**\n\n**Here, I have used CASE WHEN expression on withColumn() by using expr(), this example updates an existing column gender with the derived values, M for male, F for Female, and unknown for others**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f67137c0-d918-4514-a124-3d3439a8102f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\ncolumns = [\"name\",\"gender\"]\n\ndf = spark.createDataFrame(data, schema=columns)\n\n#Using CASE WHEN similar to SQL\ndf2 = df.withColumn(\"gender\", expr(\" CASE WHEN gender = 'M' THEN 'Male'\"+\n                                  \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"738005f2-9fbd-4cdd-b542-ab6c3f60d648","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+-------+\n|   name| gender|\n+-------+-------+\n|  James|   Male|\n|Michael| Female|\n|    Jen|unknown|\n+-------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+-------+\n|   name| gender|\n+-------+-------+\n|  James|   Male|\n|Michael| Female|\n|    Jen|unknown|\n+-------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###2.3 Using an Existing Column Value for Expression\n\n\n**Most of the PySpark function takes constant literal values but sometimes we need to use a value from an existing column instead of a constant and this is not possible without expr() expression. The below example adds a number of months from an existing column instead of a Python constant.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dcc40058-2fcf-44b7-9c02-63247e95577b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n# df = spark.createDataFrame(data, schema=['date', 'increment'])\ndf = spark.createDataFrame(data).toDF(\"date\", \"increment\")\n#Add Month value from another column\n\ndf.select(df.date, df.increment, expr(\" add_months(date, increment)\").alias(\"inc_date\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2018b810-7c40-4ad6-8d04-c72b3b5fdf47","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+---------+----------+\n|date      |increment|inc_date  |\n+----------+---------+----------+\n|2019-01-23|1        |2019-02-23|\n|2019-06-24|2        |2019-08-24|\n|2019-09-20|3        |2019-12-20|\n+----------+---------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+---------+----------+\n|date      |increment|inc_date  |\n+----------+---------+----------+\n|2019-01-23|1        |2019-02-23|\n|2019-06-24|2        |2019-08-24|\n|2019-09-20|3        |2019-12-20|\n+----------+---------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Note that Importing SQL functions are not required when using them with expr(). You see above add_months() is used without importing.**\n\n\n---\n\n\n##2.4 Giving Column Alias along with expr()\n\n\n**You can also use SQL like syntax to provide the alias name to the column expression.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8682b6b-2f58-433f-bfed-04e5ac098d36","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Providing alias using 'as'\ndf.select(df.date, df.increment, expr(\"\"\"add_months(date, increment) as inc_date\"\"\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82e85537-ab27-4a31-b836-2e641725dc12","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+---------+----------+\n|date      |increment|inc_date  |\n+----------+---------+----------+\n|2019-01-23|1        |2019-02-23|\n|2019-06-24|2        |2019-08-24|\n|2019-09-20|3        |2019-12-20|\n+----------+---------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+---------+----------+\n|date      |increment|inc_date  |\n+----------+---------+----------+\n|2019-01-23|1        |2019-02-23|\n|2019-06-24|2        |2019-08-24|\n|2019-09-20|3        |2019-12-20|\n+----------+---------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##2.5 Case Function with expr()\n\n**Below example converts long data type to String type.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ac9103f0-480d-4f62-9b46-74dad62533e8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Using Cast() Function\n\ndf.select(\"increment\", expr(\"cast(increment as string) as str_increment\"))\\\n.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"afe96aa0-cfe8-455b-b2a1-a85c431433d5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- increment: long (nullable = true)\n |-- str_increment: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- increment: long (nullable = true)\n |-- str_increment: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##2.6 Arithmetic operations\n\n**expr() is also used to provide arithmetic operations, below examples add value 5 to increment and creates a new column new_increment**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"672d817d-8c20-4bf2-9181-f16048143f3c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Arithmatic operations\ndf.select(df.date, df.increment, expr(\" increment + 5 as new_increment\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5a9a0f1e-0e0b-4de5-99b5-ec913f2f6998","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+---------+-------------+\n|      date|increment|new_increment|\n+----------+---------+-------------+\n|2019-01-23|        1|            6|\n|2019-06-24|        2|            7|\n|2019-09-20|        3|            8|\n+----------+---------+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+---------+-------------+\n|      date|increment|new_increment|\n+----------+---------+-------------+\n|2019-01-23|        1|            6|\n|2019-06-24|        2|            7|\n|2019-09-20|        3|            8|\n+----------+---------+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##2.7 Using Filter with expr()\n\n**Filter the DataFrame rows can done using expr() expression.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35460085-a0ff-4639-842b-0927ebe26427","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Use expr() to filter the rows\n\ndata = [(100,2),(200,3000),(500,500)] \n\ndf = spark.createDataFrame(data).toDF(\"col1\", \"col2\")\n\ndf.filter(expr(\"col1 == col2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bcdd1d75-53f5-421d-a7df-ce6171792701","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----+----+\n|col1|col2|\n+----+----+\n| 500| 500|\n+----+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----+----+\n|col1|col2|\n+----+----+\n| 500| 500|\n+----+----+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark – expr()","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4346796005785731}},"nbformat":4,"nbformat_minor":0}
