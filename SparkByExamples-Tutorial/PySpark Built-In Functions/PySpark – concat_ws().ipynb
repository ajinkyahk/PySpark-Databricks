{"cells":[{"cell_type":"markdown","source":["#PySpark – Convert array column to a String\n\n---\n\n**In this PySpark article, I will explain how to convert an array of String column on DataFrame to a String column (separated or concatenated with a comma, space, or any delimiter character) using PySpark function concat_ws() (translates to concat with separator), and with SQL expression using Scala example.**\n\n**When curating data on DataFrame we may want to convert the Dataframe with complex struct datatypes, arrays and maps to a flat structure. here we will see how to convert array type to string type.**\n\n**Before we start, first let’s create a DataFrame with array of string column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9edef85a-4fad-461f-a362-1ba599f88708","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n\ndata = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\ndf = spark.createDataFrame(data=data, schema=columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"22b5ae9f-4bf6-4af9-a011-c45e544b4dfd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+----------------+------------------+------------+\n|name            |languagesAtSchool |currentState|\n+----------------+------------------+------------+\n|James,,Smith    |[Java, Scala, C++]|CA          |\n|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n|Robert,,Williams|[CSharp, VB]      |NV          |\n+----------------+------------------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+----------------+------------------+------------+\n|name            |languagesAtSchool |currentState|\n+----------------+------------------+------------+\n|James,,Smith    |[Java, Scala, C++]|CA          |\n|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n|Robert,,Williams|[CSharp, VB]      |NV          |\n+----------------+------------------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**In this example “languagesAtSchool” is a column of type array. In the next section, we will convert this to a String. This example yields below schema and DataFrame.**\n\n---\n\n##Convert an array of String to String column using concat_ws()\n\n\n**In order to convert array to a string, PySpark SQL provides a built-in function concat_ws() which takes delimiter of your choice as a first argument and array column (type Column) as the second argument.**\n\n\n---\n\n\n##Syntax\n\n##concat_ws(sep, *cols)\n\n---\n\n\n###Usage\n\n**In order to use concat_ws() function, you need to import it using pyspark.sql.functions.concat_ws . Since this function takes the Column type as a second argument, you need to use col().**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"956aae4d-2d7f-4c2b-9111-52cc273de58f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, concat_ws"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3bbfb6c1-ec43-4827-afcc-269060dda841","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df2 = df.withColumn(\"languagesAtSchool\", concat_ws(\",\", col(\"languagesAtSchool\")))\n\ndf2.printSchema()\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0aabb3ab-698e-44c4-bbdd-e96ff0af070b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: string (nullable = false)\n |-- currentState: string (nullable = true)\n\n+----------------+-----------------+------------+\n|name            |languagesAtSchool|currentState|\n+----------------+-----------------+------------+\n|James,,Smith    |Java,Scala,C++   |CA          |\n|Michael,Rose,   |Spark,Java,C++   |NJ          |\n|Robert,,Williams|CSharp,VB        |NV          |\n+----------------+-----------------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: string (nullable = false)\n |-- currentState: string (nullable = true)\n\n+----------------+-----------------+------------+\n|name            |languagesAtSchool|currentState|\n+----------------+-----------------+------------+\n|James,,Smith    |Java,Scala,C++   |CA          |\n|Michael,Rose,   |Spark,Java,C++   |NJ          |\n|Robert,,Williams|CSharp,VB        |NV          |\n+----------------+-----------------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Using PySpark SQL expression\n\n\n**You can also use concat_ws() function with SQL expression.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e76e0480-472e-4471-9237-0dcc8bb151e3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.createOrReplaceTempView(\"ARRAY_STRING\")\nspark.sql(\" select name, concat_ws(',', languagesAtSchool) as languagesAtSchool, \"+\\\n         \"currentState from ARRAY_STRING\")\\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58abade0-e653-45c4-9671-558f56b8329f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------------+-----------------+------------+\n|name            |languagesAtSchool|currentState|\n+----------------+-----------------+------------+\n|James,,Smith    |Java,Scala,C++   |CA          |\n|Michael,Rose,   |Spark,Java,C++   |NJ          |\n|Robert,,Williams|CSharp,VB        |NV          |\n+----------------+-----------------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------------+-----------------+------------+\n|name            |languagesAtSchool|currentState|\n+----------------+-----------------+------------+\n|James,,Smith    |Java,Scala,C++   |CA          |\n|Michael,Rose,   |Spark,Java,C++   |NJ          |\n|Robert,,Williams|CSharp,VB        |NV          |\n+----------------+-----------------+------------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark – concat_ws()","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1848772988164696}},"nbformat":4,"nbformat_minor":0}
