{"cells":[{"cell_type":"markdown","source":["#PySpark ArrayType Column With Examples\n\n---\n\n**PySpark pyspark.sql.types.ArrayType (ArrayType extends DataType class) is used to define an array data type column on DataFrame that holds the same type of elements, In this article, I will explain how to create a DataFrame ArrayType column using org.apache.spark.sql.types.ArrayType class and applying some SQL functions on the array columns with examples.**\n\n**While working with structured files (Avro, Parquet e.t.c) or semi-structured (JSON) files, we often get data with complex structures like MapType, ArrayType, StructType e.t.c. I will try my best to cover some mostly used functions on ArrayType columns.**\n\n\n---\n\n##What is PySpark ArrayType\n\n**PySpark ArrayType is a collection data type that extends the DataType class which is a superclass of all types in PySpark. All elements of ArrayType should have the same type of elements.**\n\n---\n\n##Create PySpark ArrayType\n\n**You can create an instance of an ArrayType using ArraType() class, This takes arguments valueType and one optional argument valueContainsNull to specify if a value can accept null, by default it takes True. valueType should be a PySpark type that extends DataType class.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"461050e4-6740-4f07-8b47-d69916ea4122","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.types import StringType, ArrayType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8392904d-2e90-4a9e-ac21-006111afcad0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["arrayCol = ArrayType(StringType(), False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8c1d9b1c-62f3-40c1-8890-1a121b3170a3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Above example creates string array and doesn’t not accept null values.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"be8fbbe3-4b97-4061-9cc9-c0d167325d8a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##Create PySpark ArrayType Column Using StructType\n\n**Let’s create a DataFrame with few array columns by using PySpark StructType & StructField classes.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"16380ea5-a580-4397-8724-b4dbcebddbb4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data = [\n (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n]\n\nfrom pyspark.sql.types import StringType, ArrayType, StructType, StructField"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2b3d81c-4ae0-45d5-a828-925eb84c97b4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["schema = StructType([\n    StructField(\"name\", StringType(), True),\n    StructField(\"languagesAtSchool\", ArrayType(StringType()), True),\n    StructField(\"languagesAtWork\", ArrayType(StringType()), True),\n    StructField(\"currentState\", StringType(), True),\n    StructField(\"previousState\", StringType(), True)\n])\n\ndf = spark.createDataFrame(data=data, schema=schema)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d8e7c00c-8807-4cf6-9afe-08549ab79964","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- languagesAtWork: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n |-- previousState: string (nullable = true)\n\n+----------------+------------------+---------------+------------+-------------+\n|name            |languagesAtSchool |languagesAtWork|currentState|previousState|\n+----------------+------------------+---------------+------------+-------------+\n|James,,Smith    |[Java, Scala, C++]|[Spark, Java]  |OH          |CA           |\n|Michael,Rose,   |[Spark, Java, C++]|[Spark, Java]  |NY          |NJ           |\n|Robert,,Williams|[CSharp, VB]      |[Spark, Python]|UT          |NV           |\n+----------------+------------------+---------------+------------+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- languagesAtWork: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n |-- previousState: string (nullable = true)\n\n+----------------+------------------+---------------+------------+-------------+\n|name            |languagesAtSchool |languagesAtWork|currentState|previousState|\n+----------------+------------------+---------------+------------+-------------+\n|James,,Smith    |[Java, Scala, C++]|[Spark, Java]  |OH          |CA           |\n|Michael,Rose,   |[Spark, Java, C++]|[Spark, Java]  |NY          |NJ           |\n|Robert,,Williams|[CSharp, VB]      |[Spark, Python]|UT          |NV           |\n+----------------+------------------+---------------+------------+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##PySpark ArrayType (Array) Functions\n\n**PySpark SQL provides several Array functions to work with the ArrayType column, In this section, we will see some of the most commonly used SQL functions.**\n\n\n---\n\n\n###explode()\n\n**Use explode() function to create a new row for each element in the given array column. There are various PySpark SQL explode functions available to work with Array columns.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ae8ab90-0887-4cb7-8fb5-ed8933ec44d4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import *"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f227a32d-887c-4d0b-9bc6-19f6f278eda5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.select(df.name, explode(df.languagesAtSchool)).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"654bb921-631c-4b46-8c7d-31566e07b971","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+----------------+------+\n|name            |col   |\n+----------------+------+\n|James,,Smith    |Java  |\n|James,,Smith    |Scala |\n|James,,Smith    |C++   |\n|Michael,Rose,   |Spark |\n|Michael,Rose,   |Java  |\n|Michael,Rose,   |C++   |\n|Robert,,Williams|CSharp|\n|Robert,,Williams|VB    |\n+----------------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------------+------+\n|name            |col   |\n+----------------+------+\n|James,,Smith    |Java  |\n|James,,Smith    |Scala |\n|James,,Smith    |C++   |\n|Michael,Rose,   |Spark |\n|Michael,Rose,   |Java  |\n|Michael,Rose,   |C++   |\n|Robert,,Williams|CSharp|\n|Robert,,Williams|VB    |\n+----------------+------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Split()\n\n\n**split() sql function returns an array type after splitting the string column by delimiter. Below example split the name column by comma delimiter.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46cd4511-16f9-4dee-a65c-508266e083ba","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(split(df.name, \",\").alias(\"nameAsArray\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e691788-5e62-40ed-8e6d-588f449dd6a2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+--------------------+\n|nameAsArray         |\n+--------------------+\n|[James, , Smith]    |\n|[Michael, Rose, ]   |\n|[Robert, , Williams]|\n+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+\n|nameAsArray         |\n+--------------------+\n|[James, , Smith]    |\n|[Michael, Rose, ]   |\n|[Robert, , Williams]|\n+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#array()\n\n**Use array() function to create a new array column by merging the data from multiple columns. All input columns must have the same data type. The below example combines the data from currentState and previousState and creates a new column states.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"932d38bc-68ea-441b-a41c-407ded4c6f85","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(df.name, array(df.currentState, df.previousState).alias(\"state\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"555e27e8-7622-4042-8550-d377bc6789c0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+----------------+--------+\n|name            |state   |\n+----------------+--------+\n|James,,Smith    |[OH, CA]|\n|Michael,Rose,   |[NY, NJ]|\n|Robert,,Williams|[UT, NV]|\n+----------------+--------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------------+--------+\n|name            |state   |\n+----------------+--------+\n|James,,Smith    |[OH, CA]|\n|Michael,Rose,   |[NY, NJ]|\n|Robert,,Williams|[UT, NV]|\n+----------------+--------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##array_contains()\n\n**array_contains() sql function is used to check if array column contains a value. Returns null if the array is null, true if the array contains the value, and false otherwise.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2354e23b-e0c1-44fa-922f-9dbadb8520c1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(df.name, array_contains(df.languagesAtSchool, \"Java\").alias(\"array_contains\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10a253cc-da05-4125-9db8-da37a8a8bc35","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+----------------+--------------+\n|name            |array_contains|\n+----------------+--------------+\n|James,,Smith    |true          |\n|Michael,Rose,   |true          |\n|Robert,,Williams|false         |\n+----------------+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------------+--------------+\n|name            |array_contains|\n+----------------+--------------+\n|James,,Smith    |true          |\n|Michael,Rose,   |true          |\n|Robert,,Williams|false         |\n+----------------+--------------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark – array_contains()","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":338479643939284}},"nbformat":4,"nbformat_minor":0}
