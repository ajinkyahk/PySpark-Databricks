{"cells":[{"cell_type":"markdown","source":["#PySpark – explode nested array into rows\n\n---\n\n**Problem: How to explode & flatten nested array (Array of Array) DataFrame columns into rows using PySpark.**\n\n**Solution: PySpark explode function can be used to explode an Array of Array (nested Array) ArrayType(ArrayType(StringType)) columns to rows on PySpark DataFrame using python example.**\n\n---\n\n**Before we start, let’s create a DataFrame with a nested array column. From below example column “subjects” is an array of ArraType which holds subjects learned.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39ca7deb-d93a-4bac-ab9c-6d34b7e34eb6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import *"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b248088d-fbeb-4a33-9da7-15f48a504db8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["arrayArrayData = [\n    (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n]\n\ndf = spark.createDataFrame(data=arrayArrayData, schema=[\"name\", \"subjects\"])\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ee382ca5-dc83-40c3-b10a-5f1a2bed24e8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- subjects: array (nullable = true)\n |    |-- element: array (containsNull = true)\n |    |    |-- element: string (containsNull = true)\n\n+-------+-----------------------------------+\n|name   |subjects                           |\n+-------+-----------------------------------+\n|James  |[[Java, Scala, C++], [Spark, Java]]|\n|Michael|[[Spark, Java, C++], [Spark, Java]]|\n|Robert |[[CSharp, VB], [Spark, Python]]    |\n+-------+-----------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- subjects: array (nullable = true)\n |    |-- element: array (containsNull = true)\n |    |    |-- element: string (containsNull = true)\n\n+-------+-----------------------------------+\n|name   |subjects                           |\n+-------+-----------------------------------+\n|James  |[[Java, Scala, C++], [Spark, Java]]|\n|Michael|[[Spark, Java, C++], [Spark, Java]]|\n|Robert |[[CSharp, VB], [Spark, Python]]    |\n+-------+-----------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Now, let’s explode “subjects” array column to array rows. after exploding, it creates a new column ‘col’ with rows represents an array.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"207ac8d8-9c5e-40ce-ad1b-842790f591cf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(df.name, explode(df.subjects)).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4a59a39-0535-4192-8702-d5edf5f0fe74","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-------+------------------+\n|name   |col               |\n+-------+------------------+\n|James  |[Java, Scala, C++]|\n|James  |[Spark, Java]     |\n|Michael|[Spark, Java, C++]|\n|Michael|[Spark, Java]     |\n|Robert |[CSharp, VB]      |\n|Robert |[Spark, Python]   |\n+-------+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+------------------+\n|name   |col               |\n+-------+------------------+\n|James  |[Java, Scala, C++]|\n|James  |[Spark, Java]     |\n|Michael|[Spark, Java, C++]|\n|Michael|[Spark, Java]     |\n|Robert |[CSharp, VB]      |\n|Robert |[Spark, Python]   |\n+-------+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**If you want to flatten the arrays, use flatten function which converts array of array columns to a single array on DataFrame.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e3eb8050-790f-4b2d-8a36-50984c7ca601","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(df.name, flatten(df.subjects)).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"840b56be-0aff-419e-aa2c-e2c0e59161ba","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-------+-------------------------------+\n|name   |flatten(subjects)              |\n+-------+-------------------------------+\n|James  |[Java, Scala, C++, Spark, Java]|\n|Michael|[Spark, Java, C++, Spark, Java]|\n|Robert |[CSharp, VB, Spark, Python]    |\n+-------+-------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+-------------------------------+\n|name   |flatten(subjects)              |\n+-------+-------------------------------+\n|James  |[Java, Scala, C++, Spark, Java]|\n|Michael|[Spark, Java, C++, Spark, Java]|\n|Robert |[CSharp, VB, Spark, Python]    |\n+-------+-------------------------------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark – explode()","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":338479643939275}},"nbformat":4,"nbformat_minor":0}
