{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f874fe0-43e8-4369-bf6f-c8f66f38e747",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark Accumulator with Example\n",
    "\n",
    "---\n",
    "\n",
    "**The PySpark Accumulator is a shared variable that is used with RDD and DataFrame to perform sum and counter operations similar to Map-reduce counters. These variables are shared by all executors to update and add information through aggregation or computative operations.**\n",
    "\n",
    "---\n",
    "\n",
    "**What is PySpark Accumulator?\n",
    "Accumulators are write-only and initialize once variables where only tasks that are running on workers are allowed to update and updates from the workers get propagated automatically to the driver program. But, only the driver program is allowed to access the Accumulator variable using the value property.**\n",
    "\n",
    "---\n",
    "\n",
    "**How to create Accumulator variable in PySpark?\n",
    "Using accumulator() from SparkContext class we can create an Accumulator in PySpark programming. Users can also create Accumulators for custom types using AccumulatorParam class of PySpark.**\n",
    "\n",
    "---\n",
    "\n",
    "#####Some points to note..\n",
    "\n",
    "- sparkContext.accumulator() is used to define accumulator variables.\n",
    "- add() function is used to add/update a value in accumulator\n",
    "- value property on the accumulator variable is used to retrieve the value from the accumulator.\n",
    "\n",
    "**We can create Accumulators in PySpark for primitive types int and float. Users can also create Accumulators for custom types using AccumulatorParam class of PySpark.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60116464-113c-4aef-827a-e6367baf47a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Creating Accumulator Variable\n",
    "\n",
    "**Below is an example of how to create an accumulator variable “accum” of type int and using it to sum all values in an RDD.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf88dbb-bf8f-435a-8ddd-ca1952d8fe57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "accum = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e90e674d-18fa-48a0-b283-d58e1f924bd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c390aa-56a4-4d10-8c21-12de1bdd60d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c05e7f3-167c-4c91-93a9-d84c985fed47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Here, we have created an accumulator variable accum using spark.sparkContext.accumulator(0) with initial value 0. Later, we are iterating each element in an rdd using foreach() action and adding each element of rdd to accum variable. Finally, we are getting accumulator value using accum.value property.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "####Note that, In this example, rdd.foreach() is executed on workers and accum.value is called from PySpark driver program.\n",
    "\n",
    "**Let’s see another example of an accumulator, this time will do with a function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "081dc934-de1a-432d-9c7f-99f7fcd9ad77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "accuSum = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum += x\n",
    "    \n",
    "rdd.foreach(countFun)\n",
    "print(accuSum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f54af8e-00cc-4858-abb2-e2e2af8266f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###We can also use accumulators to do a counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50531ebd-5e6a-4d7f-8fbc-b6cac1518eb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n[[], [1], [], [2], [3], [], [4], [5]]\n"
     ]
    }
   ],
   "source": [
    "accumCount = spark.sparkContext.accumulator(0)\n",
    "\n",
    "rdd2 = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "\n",
    "rdd2.foreach(lambda x: accumCount.add(1))\n",
    "print(accumCount.value)\n",
    "print(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "636c5121-a68f-4a77-995a-2c320b5509f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Conclusion\n",
    "\n",
    "**In summary, PySpark Accumulators are shared variables that can be updated by executors and propagate back to driver program. These variables are used to add sum or counts and final results can be accessed only by driver program.**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Accumulator with Example",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
